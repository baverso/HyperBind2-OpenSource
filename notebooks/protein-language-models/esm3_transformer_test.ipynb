{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a37bdc0-933a-4c94-9464-33d5397fef2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.serialization import safe_globals\n",
    "\n",
    "from esm.models.esm3 import ESM3\n",
    "from esm.tokenization.sequence_tokenizer import EsmSequenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2453503-2177-4a8f-a87c-084bcea668eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Register the ESM3 class as safe for unpickling.\n",
    "torch.serialization.add_safe_globals([ESM3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "679b7fa5-2f4a-4af3-9dd2-419b7236dca0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "backbone_save_path = \"/home/jupyter/DATA/model_weights/esm3_backbone/esm3_backbone_model.pt\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "with safe_globals([(\"esm.models.esm3.ESM3\", ESM3)]):\n",
    "    loaded_backbone_model = torch.load(backbone_save_path, map_location=device, weights_only=False)\n",
    "    loaded_backbone_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "202f3562-e069-46f6-a5e1-95b97ced0dfa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.sequence_embed.weight: torch.Size([64, 1536])\n",
      "encoder.plddt_projection.weight: torch.Size([1536, 16])\n",
      "encoder.plddt_projection.bias: torch.Size([1536])\n",
      "encoder.structure_per_res_plddt_projection.weight: torch.Size([1536, 16])\n",
      "encoder.structure_per_res_plddt_projection.bias: torch.Size([1536])\n",
      "encoder.structure_tokens_embed.weight: torch.Size([4101, 1536])\n",
      "encoder.ss8_embed.weight: torch.Size([11, 1536])\n",
      "encoder.sasa_embed.weight: torch.Size([19, 1536])\n",
      "encoder.function_embed.0.weight: torch.Size([260, 192])\n",
      "encoder.function_embed.1.weight: torch.Size([260, 192])\n",
      "encoder.function_embed.2.weight: torch.Size([260, 192])\n",
      "encoder.function_embed.3.weight: torch.Size([260, 192])\n",
      "encoder.function_embed.4.weight: torch.Size([260, 192])\n",
      "encoder.function_embed.5.weight: torch.Size([260, 192])\n",
      "encoder.function_embed.6.weight: torch.Size([260, 192])\n",
      "encoder.function_embed.7.weight: torch.Size([260, 192])\n",
      "encoder.residue_embed.weight: torch.Size([1478, 1536])\n",
      "transformer.blocks.0.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.0.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.0.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.0.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.0.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.0.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.0.geom_attn.distance_scale_per_head: torch.Size([256])\n",
      "transformer.blocks.0.geom_attn.rotation_scale_per_head: torch.Size([256])\n",
      "transformer.blocks.0.geom_attn.s_norm.weight: torch.Size([1536])\n",
      "transformer.blocks.0.geom_attn.proj.weight: torch.Size([3840, 1536])\n",
      "transformer.blocks.0.geom_attn.out_proj.weight: torch.Size([1536, 768])\n",
      "transformer.blocks.0.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.0.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.0.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.0.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.1.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.1.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.1.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.1.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.1.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.1.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.1.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.1.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.1.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.1.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.2.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.2.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.2.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.2.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.2.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.2.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.2.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.2.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.2.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.2.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.3.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.3.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.3.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.3.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.3.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.3.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.3.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.3.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.3.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.3.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.4.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.4.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.4.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.4.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.4.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.4.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.4.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.4.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.4.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.4.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.5.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.5.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.5.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.5.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.5.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.5.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.5.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.5.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.5.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.5.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.6.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.6.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.6.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.6.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.6.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.6.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.6.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.6.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.6.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.6.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.7.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.7.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.7.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.7.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.7.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.7.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.7.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.7.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.7.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.7.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.8.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.8.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.8.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.8.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.8.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.8.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.8.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.8.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.8.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.8.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.9.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.9.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.9.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.9.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.9.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.9.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.9.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.9.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.9.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.9.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.10.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.10.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.10.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.10.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.10.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.10.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.10.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.10.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.10.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.10.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.11.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.11.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.11.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.11.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.11.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.11.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.11.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.11.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.11.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.11.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.12.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.12.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.12.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.12.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.12.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.12.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.12.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.12.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.12.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.12.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.13.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.13.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.13.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.13.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.13.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.13.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.13.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.13.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.13.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.13.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.14.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.14.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.14.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.14.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.14.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.14.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.14.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.14.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.14.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.14.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.15.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.15.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.15.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.15.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.15.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.15.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.15.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.15.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.15.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.15.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.16.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.16.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.16.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.16.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.16.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.16.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.16.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.16.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.16.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.16.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.17.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.17.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.17.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.17.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.17.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.17.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.17.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.17.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.17.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.17.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.18.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.18.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.18.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.18.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.18.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.18.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.18.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.18.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.18.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.18.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.19.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.19.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.19.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.19.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.19.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.19.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.19.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.19.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.19.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.19.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.20.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.20.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.20.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.20.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.20.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.20.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.20.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.20.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.20.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.20.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.21.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.21.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.21.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.21.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.21.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.21.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.21.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.21.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.21.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.21.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.22.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.22.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.22.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.22.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.22.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.22.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.22.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.22.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.22.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.22.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.23.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.23.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.23.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.23.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.23.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.23.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.23.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.23.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.23.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.23.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.24.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.24.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.24.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.24.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.24.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.24.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.24.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.24.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.24.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.24.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.25.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.25.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.25.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.25.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.25.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.25.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.25.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.25.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.25.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.25.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.26.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.26.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.26.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.26.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.26.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.26.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.26.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.26.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.26.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.26.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.27.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.27.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.27.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.27.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.27.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.27.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.27.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.27.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.27.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.27.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.28.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.28.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.28.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.28.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.28.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.28.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.28.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.28.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.28.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.28.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.29.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.29.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.29.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.29.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.29.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.29.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.29.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.29.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.29.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.29.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.30.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.30.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.30.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.30.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.30.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.30.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.30.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.30.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.30.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.30.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.31.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.31.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.31.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.31.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.31.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.31.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.31.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.31.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.31.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.31.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.32.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.32.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.32.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.32.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.32.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.32.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.32.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.32.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.32.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.32.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.33.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.33.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.33.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.33.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.33.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.33.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.33.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.33.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.33.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.33.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.34.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.34.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.34.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.34.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.34.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.34.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.34.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.34.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.34.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.34.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.35.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.35.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.35.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.35.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.35.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.35.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.35.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.35.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.35.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.35.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.36.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.36.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.36.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.36.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.36.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.36.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.36.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.36.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.36.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.36.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.37.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.37.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.37.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.37.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.37.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.37.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.37.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.37.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.37.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.37.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.38.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.38.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.38.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.38.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.38.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.38.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.38.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.38.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.38.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.38.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.39.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.39.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.39.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.39.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.39.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.39.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.39.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.39.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.39.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.39.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.40.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.40.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.40.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.40.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.40.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.40.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.40.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.40.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.40.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.40.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.41.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.41.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.41.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.41.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.41.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.41.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.41.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.41.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.41.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.41.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.42.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.42.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.42.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.42.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.42.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.42.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.42.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.42.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.42.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.42.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.43.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.43.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.43.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.43.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.43.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.43.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.43.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.43.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.43.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.43.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.44.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.44.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.44.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.44.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.44.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.44.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.44.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.44.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.44.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.44.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.45.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.45.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.45.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.45.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.45.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.45.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.45.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.45.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.45.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.45.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.46.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.46.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.46.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.46.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.46.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.46.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.46.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.46.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.46.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.46.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.47.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.47.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.47.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.47.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.47.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.47.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.47.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.47.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.47.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.47.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.norm.weight: torch.Size([1536])\n",
      "Total number of parameters: 1375643648\n"
     ]
    }
   ],
   "source": [
    "state_dict = loaded_backbone_model.state_dict()\n",
    "for key, tensor in state_dict.items():\n",
    "    print(f\"{key}: {tensor.shape}\")\n",
    "    \n",
    "total_params = sum(p.numel() for p in loaded_backbone_model.parameters())\n",
    "print(\"Total number of parameters:\", total_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d7d61c-5797-4369-ba1d-d0d6a12d8f04",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Explainer: Transformer Block with Geometric Attention in ESM3\n",
    "\n",
    "In the ESM3 architecture, the **first transformer block** is unique because it integrates a specialized module for geometric reasoning, which is crucial for capturing structural relationships (e.g., in protein data).\n",
    "```\n",
    "TransformerStack(\n",
    "  (blocks): ModuleList(\n",
    "    (0): UnifiedTransformerBlock(\n",
    "      (attn): MultiHeadAttention(\n",
    "        (layernorm_qkv): Sequential(\n",
    "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
    "          (1): Linear(in_features=1536, out_features=4608, bias=False)\n",
    "        )\n",
    "        (out_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
    "        (q_ln): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
    "        (k_ln): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
    "        (rotary): RotaryEmbedding()\n",
    "      )\n",
    "      (geom_attn): GeometricReasoningOriginalImpl(\n",
    "        (s_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
    "        (proj): Linear(in_features=1536, out_features=3840, bias=False)\n",
    "        (out_proj): Linear(in_features=768, out_features=1536, bias=False)\n",
    "      )\n",
    "      (ffn): Sequential(\n",
    "        (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
    "        (1): Linear(in_features=1536, out_features=8192, bias=False)\n",
    "        (2): SwiGLU()\n",
    "        (3): Linear(in_features=4096, out_features=1536, bias=False)\n",
    "      )\n",
    "    )\n",
    "    (1-47): 47 x UnifiedTransformerBlock(\n",
    "      (attn): MultiHeadAttention(\n",
    "        (layernorm_qkv): Sequential(\n",
    "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
    "          (1): Linear(in_features=1536, out_features=4608, bias=False)\n",
    "        )\n",
    "        (out_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
    "        (q_ln): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
    "        (k_ln): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
    "        (rotary): RotaryEmbedding()\n",
    "      )\n",
    "      (ffn): Sequential(\n",
    "        (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
    "        (1): Linear(in_features=1536, out_features=8192, bias=False)\n",
    "        (2): SwiGLU()\n",
    "        (3): Linear(in_features=4096, out_features=1536, bias=False)\n",
    "      )\n",
    "    )\n",
    "  )\n",
    "  (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
    "```\n",
    "\n",
    "Below is a breakdown of its components:\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Multi-Head Self-Attention (`attn`)\n",
    "\n",
    "- **Purpose:**  \n",
    "  Processes the input sequence by allowing each token to attend to every other token.\n",
    "\n",
    "- **Key Components:**  \n",
    "  - **Layer Normalization & Linear Projections:**  \n",
    "    Prepares the queries, keys, and values for attention computation.\n",
    "  - **Rotary Embeddings:**  \n",
    "    Enhances the attention mechanism by incorporating relative positional information.\n",
    "  - **Output Projection:**  \n",
    "    Consolidates the attention outputs back into the original embedding space.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Geometric Attention (`geom_attn`)\n",
    "\n",
    "- **Purpose:**  \n",
    "  Enhances the transformer block by integrating geometric or structural information. This is particularly important for applications like protein structure prediction where spatial relationships are key.\n",
    "\n",
    "- **Key Components:**\n",
    "  - **Layer Normalization (`s_norm`):**  \n",
    "    Normalizes the input prior to geometric processing.\n",
    "  - **Projection (`proj`):**  \n",
    "    Maps the 1536-dimensional input into a higher-dimensional space (3840 dimensions) to capture more complex geometric features.\n",
    "  - **Output Projection (`out_proj`):**  \n",
    "    Reduces the dimensionality from 768 (after geometric processing) back to 1536, ensuring compatibility with the rest of the network.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Feed-Forward Network (`ffn`)\n",
    "\n",
    "- **Purpose:**  \n",
    "  Applies further non-linear transformations to the output from the attention mechanisms.\n",
    "\n",
    "- **Key Components:**\n",
    "  - **Layer Normalization:**  \n",
    "    Prepares the data for the feed-forward operations.\n",
    "  - **Intermediate Linear Layer (Expansion to 8192 dimensions):**  \n",
    "    Expands the feature space to allow for complex representations.\n",
    "  - **SwiGLU Activation:**  \n",
    "    A non-linear activation that enhances the modelâ€™s expressivity.\n",
    "  - **Final Linear Projection:**  \n",
    "    Compresses the expanded representation back to the original 1536 dimensions.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "The **first transformer block** in ESM3 is a **UnifiedTransformerBlock** that stands out due to the integration of **geometric attention**. This extra module (`geom_attn`) augments the standard self-attention mechanism by incorporating geometric reasoning, making the block adept at handling structural data alongside sequence information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579ab006-ede5-4890-ae35-a52a2bca6f60",
   "metadata": {},
   "source": [
    "# esm encoder+transformer block unit testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21256c93-060e-4f20-a7dc-c73cbd0d4709",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from esm.tokenization.sequence_tokenizer import EsmSequenceTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "671e3703-6acc-470d-8148-a80dac4401c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from esm.sdk.api import ESMProtein, ProteinComplex\n",
    "from esm.utils.structure.protein_chain import ProteinChain\n",
    "from esm.utils.types import FunctionAnnotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2a8c5aa-0460-4eb4-9f3a-735a96634cc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fpath = '/home/jupyter/1BEY.pdb'\n",
    "protein_chain = ProteinChain.from_pdb(fpath, chain_id='H')\n",
    "monomer_protein = ESMProtein.from_protein_chain(protein_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ea7400b-1271-43e0-a457-259d8db31211",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "complex = ProteinComplex.from_pdb(fpath)\n",
    "multimer_protein = ESMProtein.from_protein_complex(complex)\n",
    "protein = multimer_protein # for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a815da9b-d5f9-4d38-8040-94be6f87b543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7e506c-f0df-43ab-ab7b-ac6fa47d1411",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55ece0a-d662-4185-8a41-e44ff1c4a3cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0224a21a-37c7-496f-a19e-e8892bb666bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We must pass zero vectors for input modalities with null\n",
    "\n",
    "# Dummy structure tokens\n",
    "dummy_structure_tokens = torch.zeros_like(seq_tokens_tensor, dtype=torch.int64, device=device)\n",
    "\n",
    "# Dummy tokens for ss8 (secondary structure, 11 classes) and SASA (solvent accessibility, 19 classes)\n",
    "dummy_ss8_tokens = torch.zeros_like(seq_tokens_tensor, dtype=torch.int64, device=device)\n",
    "dummy_sasa_tokens = torch.zeros_like(seq_tokens_tensor, dtype=torch.int64, device=device)\n",
    "\n",
    "# Dummy pLDDT values (typically float values per residue)\n",
    "dummy_average_plddt = torch.ones_like(seq_tokens_tensor, dtype=torch.float32, device=device)\n",
    "dummy_per_res_plddt = torch.ones_like(seq_tokens_tensor, dtype=torch.float32, device=device)\n",
    "\n",
    "batch_size, seq_len = seq_tokens_tensor.shape\n",
    "dummy_rbf = torch.ones(batch_size, seq_len, 16,\n",
    "                       dtype=loaded_backbone_model.encoder.plddt_projection.weight.dtype,\n",
    "                       device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "157cc9ef-869a-4af9-9452-3d87006115f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence embeddings shape: torch.Size([1, 427, 1536])\n",
      "tensor([[[ 0.0776, -0.0176,  0.0410,  ..., -0.0295, -0.0197, -0.0006],\n",
      "         [ 0.0859, -0.1328, -0.0781,  ..., -0.0317,  0.0208, -0.0364],\n",
      "         [-0.0033, -0.0562,  0.0261,  ..., -0.0188, -0.0027, -0.0928],\n",
      "         ...,\n",
      "         [ 0.1367, -0.0083,  0.0413,  ...,  0.0200,  0.1426, -0.0144],\n",
      "         [ 0.0132, -0.0060,  0.1436,  ..., -0.0309, -0.1348, -0.0116],\n",
      "         [ 0.0598,  0.0299,  0.0173,  ..., -0.0312, -0.0217, -0.0162]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# TEST SEQUENCE EMBEDDINGS\n",
    "with torch.no_grad():\n",
    "    embeddings = loaded_backbone_model.encoder.sequence_embed(seq_tokens_tensor)\n",
    "print(\"Sequence embeddings shape:\", embeddings.shape)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c4744f1a-9697-4206-95cb-89d745a599b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure tokens embedding shape: torch.Size([1, 17, 1536])\n",
      "tensor([[[ 0.0454,  0.1699,  0.1035,  ..., -0.1133, -0.0625,  0.0530],\n",
      "         [ 0.0454,  0.1699,  0.1035,  ..., -0.1133, -0.0625,  0.0530],\n",
      "         [ 0.0454,  0.1699,  0.1035,  ..., -0.1133, -0.0625,  0.0530],\n",
      "         ...,\n",
      "         [ 0.0454,  0.1699,  0.1035,  ..., -0.1133, -0.0625,  0.0530],\n",
      "         [ 0.0454,  0.1699,  0.1035,  ..., -0.1133, -0.0625,  0.0530],\n",
      "         [ 0.0454,  0.1699,  0.1035,  ..., -0.1133, -0.0625,  0.0530]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# TEST STRUCTURE TOKENS EMBEDDINGS\n",
    "# Create dummy structure tokens (all zeros, matching the sequence shape)\n",
    "dummy_structure_tokens = torch.zeros_like(seq_tokens_tensor, dtype=torch.int64, device=device)\n",
    "with torch.no_grad():\n",
    "    structure_embedding = loaded_backbone_model.encoder.structure_tokens_embed(dummy_structure_tokens)\n",
    "print(\"Structure tokens embedding shape:\", structure_embedding.shape)\n",
    "print(structure_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3503f9b4-5f97-49b4-aa48-6ac7dff75000",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pLDDT projection output shape: torch.Size([1, 17, 1536])\n",
      "tensor([[[ 1.0391,  0.9102, -0.0137,  ..., -0.2275, -1.1875,  1.2891],\n",
      "         [ 1.0391,  0.9102, -0.0137,  ..., -0.2275, -1.1875,  1.2891],\n",
      "         [ 1.0391,  0.9102, -0.0137,  ..., -0.2275, -1.1875,  1.2891],\n",
      "         ...,\n",
      "         [ 1.0391,  0.9102, -0.0137,  ..., -0.2275, -1.1875,  1.2891],\n",
      "         [ 1.0391,  0.9102, -0.0137,  ..., -0.2275, -1.1875,  1.2891],\n",
      "         [ 1.0391,  0.9102, -0.0137,  ..., -0.2275, -1.1875,  1.2891]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# TEST PLDDT PROJECTION\n",
    "# Create a dummy input for the pLDDT projection module (shape: batch_size x seq_len x 16)\n",
    "batch_size, seq_len = seq_tokens_tensor.shape\n",
    "dummy_rbf = torch.ones(\n",
    "    batch_size,\n",
    "    seq_len,\n",
    "    16,\n",
    "    dtype=loaded_backbone_model.encoder.plddt_projection.weight.dtype,\n",
    "    device=device\n",
    ")\n",
    "with torch.no_grad():\n",
    "    plddt_embedding = loaded_backbone_model.encoder.plddt_projection(dummy_rbf)\n",
    "print(\"pLDDT projection output shape:\", plddt_embedding.shape)\n",
    "print(plddt_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d14d1a37-adfe-4834-9f9b-93d30270e150",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SS8 embedding shape: torch.Size([1, 17, 1536])\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# TEST SS8 EMBEDDING\n",
    "# Create dummy tokens for SS8 (secondary structure; 11 classes)\n",
    "dummy_ss8_tokens = torch.zeros_like(seq_tokens_tensor, dtype=torch.int64, device=device)\n",
    "with torch.no_grad():\n",
    "    ss8_embedding = loaded_backbone_model.encoder.ss8_embed(dummy_ss8_tokens)\n",
    "print(\"SS8 embedding shape:\", ss8_embedding.shape)\n",
    "print(ss8_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5b82b6f0-adea-4c4f-beab-c9cd038a7262",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SASA embedding shape: torch.Size([1, 17, 1536])\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# TEST SASA EMBEDDING\n",
    "# Create dummy tokens for SASA (solvent accessibility; 19 classes)\n",
    "dummy_sasa_tokens = torch.zeros_like(seq_tokens_tensor, dtype=torch.int64, device=device)\n",
    "with torch.no_grad():\n",
    "    sasa_embedding = loaded_backbone_model.encoder.sasa_embed(dummy_sasa_tokens)\n",
    "print(\"SASA embedding shape:\", sasa_embedding.shape)\n",
    "print(sasa_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "952e2100-79ce-4625-82bb-deddb5ed8508",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function embedding 0 shape: torch.Size([1, 17, 192])\n",
      "tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.2812,  0.1147, -0.4746,  ...,  0.1230, -0.6133,  0.3711],\n",
      "         [-0.9922, -0.3828,  0.4082,  ...,  0.4238, -0.2676,  0.6367],\n",
      "         ...,\n",
      "         [-0.3105,  0.6406, -0.7344,  ..., -0.4609,  0.2061,  0.7539],\n",
      "         [-0.1162, -0.0476,  0.5820,  ..., -0.3125, -0.2734, -0.3555],\n",
      "         [-0.2480,  0.2949, -0.6211,  ...,  0.3691,  0.1768, -0.2119]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "Function embedding 1 shape: torch.Size([1, 17, 192])\n",
      "tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.3535, -0.1108,  0.1235,  ...,  0.5859,  0.1719, -0.1631],\n",
      "         [ 0.0452,  0.4883, -0.4570,  ...,  0.4160, -0.3770, -0.2305],\n",
      "         ...,\n",
      "         [ 0.9102,  0.6133,  0.3770,  ...,  0.2148,  0.0776, -1.0000],\n",
      "         [-0.6211,  0.0542, -0.7031,  ..., -0.0571, -0.3672,  0.0087],\n",
      "         [-0.1553, -0.3516,  0.2656,  ...,  0.4727, -0.4297,  0.5312]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "Function embedding 2 shape: torch.Size([1, 17, 192])\n",
      "tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.4531, -0.0102,  0.4590,  ..., -0.8711,  0.7344, -1.1094],\n",
      "         [-0.6250,  0.4199,  0.4570,  ..., -0.6328,  0.7617,  0.5312],\n",
      "         ...,\n",
      "         [-0.0391,  0.3457,  0.4902,  ..., -0.8984, -0.1045,  0.5547],\n",
      "         [-0.5781,  0.3184,  0.5234,  ...,  0.9688,  0.8867,  0.6250],\n",
      "         [ 0.1553, -0.4785, -0.5234,  ...,  0.0718, -0.6875,  0.1904]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "Function embedding 3 shape: torch.Size([1, 17, 192])\n",
      "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [-4.0430e-01,  4.6680e-01, -9.8438e-01,  ...,  9.1309e-02,\n",
      "          -4.1797e-01,  1.0000e+00],\n",
      "         [-8.3984e-01,  8.6328e-01,  7.1484e-01,  ..., -2.7734e-01,\n",
      "           6.3281e-01, -4.9023e-01],\n",
      "         ...,\n",
      "         [ 1.4922e+00,  3.7891e-01,  5.7422e-01,  ...,  5.1562e-01,\n",
      "           6.5234e-01,  6.9922e-01],\n",
      "         [ 1.8359e-01, -2.0605e-01, -8.9844e-01,  ...,  8.5547e-01,\n",
      "           1.0938e+00,  9.4922e-01],\n",
      "         [-5.8594e-01, -5.3516e-01, -4.8065e-04,  ...,  4.2578e-01,\n",
      "          -2.2949e-02, -1.8457e-01]]], device='cuda:0', dtype=torch.bfloat16)\n",
      "Function embedding 4 shape: torch.Size([1, 17, 192])\n",
      "tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.1865, -0.1445, -0.0466,  ...,  0.7305, -0.2305,  0.0398],\n",
      "         [ 0.7422, -0.2539, -0.7109,  ..., -0.4277,  0.0864,  0.0147],\n",
      "         ...,\n",
      "         [ 0.8594, -0.6523, -0.5430,  ..., -0.0913, -0.8555, -0.7109],\n",
      "         [ 0.0298, -0.3926,  0.3848,  ...,  0.0820, -1.6250,  0.3477],\n",
      "         [-0.4180, -0.5469,  0.0747,  ..., -0.0757, -0.2598, -0.3457]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "Function embedding 5 shape: torch.Size([1, 17, 192])\n",
      "tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.0262, -0.4414,  0.5273,  ..., -0.4590, -1.1328, -1.3594],\n",
      "         [ 0.1348,  0.1602, -0.6836,  ..., -0.6211,  0.6445,  0.6758],\n",
      "         ...,\n",
      "         [-1.5078,  0.7344,  0.3887,  ..., -0.2461,  0.1177,  0.4180],\n",
      "         [ 0.8594, -0.1289, -0.5430,  ...,  0.6055, -0.1797,  0.0349],\n",
      "         [ 0.7344, -0.0718,  0.2129,  ..., -0.4023, -0.5469, -0.6719]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "Function embedding 6 shape: torch.Size([1, 17, 192])\n",
      "tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.2227,  0.2617, -0.4082,  ..., -0.4434, -0.4023,  0.1816],\n",
      "         [ 0.0645,  0.4668,  0.0231,  ...,  0.5117, -0.9453,  0.3086],\n",
      "         ...,\n",
      "         [ 1.2188,  0.0977,  0.3516,  ...,  0.2070,  0.6641, -0.4258],\n",
      "         [-0.5625,  0.8008,  0.5508,  ...,  0.1177, -1.2734,  0.3281],\n",
      "         [-0.4102, -0.1621, -0.7422,  ...,  0.3301, -0.0659, -0.5586]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "Function embedding 7 shape: torch.Size([1, 17, 192])\n",
      "tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.2930, -0.6172, -0.0393,  ...,  0.4434, -0.2715,  0.1377],\n",
      "         [ 0.0162, -0.6680,  0.3809,  ..., -0.9297,  0.1543, -0.1641],\n",
      "         ...,\n",
      "         [ 0.5859,  0.3535,  0.3770,  ...,  0.7344, -0.2412, -0.5352],\n",
      "         [ 0.0500,  0.6602,  0.3125,  ..., -0.6484,  0.7461,  1.1641],\n",
      "         [ 0.0258,  0.4141,  0.5547,  ..., -0.3711,  0.2393,  0.1494]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# TEST FUNCTION EMBEDDINGS\n",
    "# The function_embed module is a ModuleList; test each embedding layer.\n",
    "for i, func_embed in enumerate(loaded_backbone_model.encoder.function_embed):\n",
    "    with torch.no_grad():\n",
    "        func_embedding = func_embed(seq_tokens_tensor)\n",
    "    print(f\"Function embedding {i} shape:\", func_embedding.shape)\n",
    "    print(func_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bec9f833-0d3e-42fe-9921-af5ddbe50215",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residue embedding shape: torch.Size([1, 1536])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# TEST RESIDUE EMBEDDING\n",
    "# The residue embedding is an EmbeddingBag, so we flatten the input and provide offsets.\n",
    "dummy_residue_tokens = torch.zeros(seq_tokens_tensor.shape, dtype=torch.int64, device=device)\n",
    "dummy_residue_tokens_flat = dummy_residue_tokens.view(-1)\n",
    "offsets = torch.tensor([0], dtype=torch.int64, device=device)\n",
    "with torch.no_grad():\n",
    "    residue_embedding = loaded_backbone_model.encoder.residue_embed(dummy_residue_tokens_flat, offsets)\n",
    "print(\"Residue embedding shape:\", residue_embedding.shape)\n",
    "print(residue_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a4d468-33d2-4402-8891-ee650670af46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b3ea49-1520-46af-9098-08f1331bd841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dd7911-f879-4c02-ac8c-7ac00d3c301d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c09042-5d18-480b-80bf-4827fac91ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ca8e40-53bf-43bd-965b-8a7a19af6e48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b243f8-8211-491d-93d9-b7ca7097e6d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42da637-1ff1-44c7-927e-f27afb717393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518ae2f7-1991-48b4-a8c9-53906eddbe2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "416b0071-24a0-43c2-901e-190dbbdd95db",
   "metadata": {
    "tags": []
   },
   "source": [
    "structure_embedding = loaded_backbone_model.encoder.structure_tokens_embed(structure_tokens)\n",
    "print(\"Structure tokens embedding shape:\", structure_embedding.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268b9f67-f5c4-4b43-a90a-1ecd27aeb6fe",
   "metadata": {},
   "source": [
    "# objectives:\n",
    "# 1. validate other embeddings work (may need antibody pdbs as input)\n",
    "# 2. validate embedding dimensions / shape\n",
    "# 3. Run multiple sequences in one pass?\n",
    "# 4. Speed test 10, 100, 1000 sequences runtime\n",
    "# 5. Training...OPT 3 guide a fine-tuning of this model (on 5 random antibody sequences)\n",
    "# 6. LORA for training econo.y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615d1ebe-6438-490b-b6ed-fcb0f5c3213c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e9c9a7-6aa2-4bdb-a3ee-21dced1e13a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13 (Local)",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
