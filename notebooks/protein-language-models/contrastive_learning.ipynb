{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Contrastive Learning Model for Protein Sequence Pairs Using ESM3\n",
       "\n",
       "This notebook demonstrates an enhanced contrastive learning model for protein sequences using the ESM3 model as a backbone. We implement a custom attention-based adapter, a projection head, and utility functions for tokenization, data loading, and inference."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 1. Imports and Dependencies\n",
       "We start by importing all required libraries, including PyTorch, pandas, numpy, sklearn metrics, and ESM3."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "import os\n",
       "import torch\n",
       "import torch.nn as nn\n",
       "import torch.nn.functional as F\n",
       "from torch.utils.data import Dataset, DataLoader\n",
       "import pandas as pd\n",
       "import numpy as np\n",
       "from sklearn.model_selection import train_test_split\n",
       "from sklearn.metrics import (\n",
       "    roc_auc_score,\n",
       "    precision_recall_curve,\n",
       "    auc,\n",
       "    roc_curve\n",
       ")\n",
       "import matplotlib.pyplot as plt\n",
       "\n",
       "# ESM3 imports\n",
       "from esm.pretrained import ESM3_sm_open_v0"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 2. Custom ESM3 Adapter and Contrastive Model Classes\n",
       "Here we define:\n",
       "- **CustomESM3Adapter**: Wraps the ESM3 model and applies attention-based pooling.\n",
       "- **ImprovedContrastiveHead**: A projection head with residual connections and normalization.\n",
       "- **EnhancedContrastiveModel**: Combines the adapter and the projection head and provides a learnable temperature parameter."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "class CustomESM3Adapter(nn.Module):\n",
       "    \"\"\"Improved adapter for ESM3 with attention-based pooling.\"\"\"\n",
       "\n",
       "    def __init__(self, esm3_model):\n",
       "        \"\"\"Initialize the adapter with the ESM3 model.\n",
       "        \n",
       "        Args:\n",
       "            esm3_model: An ESM3 model instance\n",
       "        \"\"\"\n",
       "        super().__init__()\n",
       "        self.model = esm3_model\n",
       "        self.embed_dim = 1536  # Default for ESM3-small\n",
       "\n",
       "        # Add attention-based pooling\n",
       "        self.attention = nn.Sequential(\n",
       "            nn.Linear(self.embed_dim, 512),\n",
       "            nn.Tanh(),\n",
       "            nn.Linear(512, 1),\n",
       "            nn.Softmax(dim=1)\n",
       "        )\n",
       "\n",
       "    def forward(self, tokens):\n",
       "        \"\"\"Process sequence tokens and return embeddings with attention pooling.\n",
       "        \n",
       "        Args:\n",
       "            tokens: Tokenized protein sequences\n",
       "\n",
       "        Returns:\n",
       "            torch.Tensor: Pooled embeddings\n",
       "        \"\"\"\n",
       "        # Ensure tokens is a long tensor\n",
       "        tokens = tokens.long()\n",
       "\n",
       "        # Create attention mask (1 for real tokens, 0 for padding)\n",
       "        padding_mask = (tokens != 1)  # 1 is pad_idx\n",
       "\n",
       "        # Forward pass through ESM3 model\n",
       "        with torch.no_grad():\n",
       "            result = self.model(sequence_tokens=tokens)\n",
       "\n",
       "            # Extract embeddings\n",
       "            if hasattr(result, 'embeddings'):\n",
       "                embeddings = result.embeddings\n",
       "            else:\n",
       "                raise ValueError(\"No embeddings found in model output\")\n",
       "\n",
       "        # Apply attention-based pooling if embeddings have sequence dimension\n",
       "        if embeddings.dim() == 3:  # [batch_size, seq_len, hidden_dim]\n",
       "            # Mask out padding tokens\n",
       "            mask = padding_mask.unsqueeze(-1).float()\n",
       "\n",
       "            # Calculate attention weights\n",
       "            attention_weights = self.attention(embeddings)\n",
       "\n",
       "            # Apply mask to attention weights\n",
       "            attention_weights = attention_weights * mask\n",
       "            attention_weights = attention_weights / (\n",
       "                attention_weights.sum(dim=1, keepdim=True) + 1e-8\n",
       "            )\n",
       "\n",
       "            # Weighted sum of token embeddings\n",
       "            pooled_embeddings = torch.sum(\n",
       "                embeddings * attention_weights, dim=1\n",
       "            )\n",
       "            return pooled_embeddings\n",
       "        else:\n",
       "            return embeddings\n",
       "\n",
       "\n",
       "class ImprovedContrastiveHead(nn.Module):\n",
       "    \"\"\"Improved projection head with residual connections and normalization.\"\"\"\n",
       "\n",
       "    def __init__(self, in_dim=1536, hidden_dim=1024, out_dim=256, dropout=0.2):\n",
       "        \"\"\"Initialize projection head.\n",
       "        \n",
       "        Args:\n",
       "            in_dim: Input embedding dimension\n",
       "            hidden_dim: Hidden layer dimension\n",
       "            out_dim: Output projection dimension\n",
       "            dropout: Dropout rate\n",
       "        \"\"\"\n",
       "        super().__init__()\n",
       "\n",
       "        # First block\n",
       "        self.block1 = nn.Sequential(\n",
       "            nn.Linear(in_dim, hidden_dim),\n",
       "            nn.LayerNorm(hidden_dim),\n",
       "            nn.GELU(),\n",
       "            nn.Dropout(dropout)\n",
       "        )\n",
       "\n",
       "        # Second block with residual connection\n",
       "        self.block2 = nn.Sequential(\n",
       "            nn.Linear(hidden_dim, hidden_dim),\n",
       "            nn.LayerNorm(hidden_dim),\n",
       "            nn.GELU(),\n",
       "            nn.Dropout(dropout)\n",
       "        )\n",
       "\n",
       "        # Output projection\n",
       "        self.output = nn.Linear(hidden_dim, out_dim)\n",
       "\n",
       "        # Skip connection adapter (if input and hidden dimensions differ)\n",
       "        self.skip_adapter = (\n",
       "            nn.Linear(in_dim, hidden_dim)\n",
       "            if in_dim != hidden_dim else nn.Identity()\n",
       "        )\n",
       "\n",
       "    def forward(self, x):\n",
       "        \"\"\"Project embeddings to a lower-dimensional space.\n",
       "        \n",
       "        Args:\n",
       "            x: Input embeddings\n",
       "\n",
       "        Returns:\n",
       "            torch.Tensor: Normalized projections\n",
       "        \"\"\"\n",
       "        # First block\n",
       "        block1_out = self.block1(x)\n",
       "\n",
       "        # Second block with residual connection\n",
       "        block2_out = self.block2(block1_out) + block1_out\n",
       "\n",
       "        # Output projection\n",
       "        projected = self.output(block2_out)\n",
       "\n",
       "        # Normalize to unit length\n",
       "        normalized = F.normalize(projected, p=2, dim=1)\n",
       "        return normalized\n",
       "\n",
       "\n",
       "class EnhancedContrastiveModel(nn.Module):\n",
       "    \"\"\"Enhanced contrastive model with improved architecture.\"\"\"\n",
       "\n",
       "    def __init__(self, esm3_model, freeze_backbone=True, temperature=0.07):\n",
       "        \"\"\"Initialize the contrastive model.\n",
       "        \n",
       "        Args:\n",
       "            esm3_model: ESM3 model to use as backbone\n",
       "            freeze_backbone: Whether to freeze backbone weights\n",
       "            temperature: Initial temperature for similarity scaling\n",
       "        \"\"\"\n",
       "        super().__init__()\n",
       "\n",
       "        # Create backbone with attention-based pooling\n",
       "        self.backbone = CustomESM3Adapter(esm3_model)\n",
       "        self.embed_dim = 1536  # ESM3-small embedding dimension\n",
       "\n",
       "        # Create improved projection head\n",
       "        self.projection_head = ImprovedContrastiveHead(\n",
       "            in_dim=self.embed_dim,\n",
       "            hidden_dim=1024,\n",
       "            out_dim=256,\n",
       "            dropout=0.3\n",
       "        )\n",
       "\n",
       "        # Learnable temperature parameter for similarity scaling\n",
       "        self.temperature = nn.Parameter(torch.tensor(temperature))\n",
       "\n",
       "        # Freeze backbone if requested\n",
       "        if freeze_backbone:\n",
       "            for param in self.backbone.model.parameters():\n",
       "                param.requires_grad = False\n",
       "\n",
       "        # Selectively unfreeze the attention mechanism in the adapter\n",
       "        for param in self.backbone.attention.parameters():\n",
       "            param.requires_grad = True\n",
       "\n",
       "    def forward(self, sequence_tokens):\n",
       "        \"\"\"Forward pass through the enhanced model.\n",
       "        \n",
       "        Args:\n",
       "            sequence_tokens: Tokenized protein sequences\n",
       "\n",
       "        Returns:\n",
       "            torch.Tensor: Projected embeddings\n",
       "        \"\"\"\n",
       "        # Get embeddings with attention pooling\n",
       "        embeddings = self.backbone(sequence_tokens)\n",
       "\n",
       "        # Project to lower-dimensional space\n",
       "        projected = self.projection_head(embeddings)\n",
       "\n",
       "        return projected\n",
       "\n",
       "    def compute_similarity(self, z1, z2):\n",
       "        \"\"\"Compute cosine similarity with learnable temperature.\n",
       "        \n",
       "        Args:\n",
       "            z1: First set of embeddings\n",
       "            z2: Second set of embeddings\n",
       "\n",
       "        Returns:\n",
       "            torch.Tensor: Similarity scores\n",
       "        \"\"\"\n",
       "        # Cosine similarity with temperature scaling\n",
       "        sim = F.cosine_similarity(z1, z2, dim=1) / self.temperature\n",
       "        return sim\n"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 3. Helper Tokenization and Dataset Classes\n",
       "These classes handle sequence tokenization and dataset creation:\n",
       "- **tokenize_sequence**: A convenience function to tokenize a single sequence.\n",
       "- **DirectContrastiveDataset**: A dataset that reads raw strings (CDR3 and antigen) and tokenizes them for contrastive learning."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def tokenize_sequence(sequence, max_length=384):\n",
       "    \"\"\"Tokenize a protein sequence for ESM3.\n",
       "\n",
       "    Args:\n",
       "        sequence (str): Amino acid sequence\n",
       "        max_length (int): Maximum sequence length\n",
       "\n",
       "    Returns:\n",
       "        torch.Tensor: Tokenized sequence\n",
       "    \"\"\"\n",
       "    # ESM3 token mapping\n",
       "    aa_to_idx = {\n",
       "        'A': 4, 'R': 5, 'N': 6, 'D': 7, 'C': 8, 'Q': 9, 'E': 10, 'G': 11,\n",
       "        'H': 12, 'I': 13, 'L': 14, 'K': 15, 'M': 16, 'F': 17, 'P': 18,\n",
       "        'S': 19, 'T': 20, 'W': 21, 'Y': 22, 'V': 23, 'B': 24, 'J': 25,\n",
       "        'O': 26, 'U': 27, 'X': 28, 'Z': 29, '.': 30, '-': 31, '|': 31\n",
       "    }\n",
       "\n",
       "    # Special tokens\n",
       "    cls_idx = 0  # Beginning of sequence\n",
       "    pad_idx = 1  # Padding token\n",
       "    eos_idx = 2  # End of sequence\n",
       "    unk_idx = 3  # Unknown amino acid\n",
       "\n",
       "    # Start with <cls>\n",
       "    token_ids = [cls_idx]\n",
       "\n",
       "    # Tokenize sequence\n",
       "    for aa in sequence:\n",
       "        # Skip invalid characters\n",
       "        if aa in ' \\t\\n\\r':\n",
       "            continue\n",
       "        # Convert to uppercase and get token ID\n",
       "        token_ids.append(aa_to_idx.get(aa.upper(), unk_idx))\n",
       "\n",
       "    # Add end of sequence\n",
       "    token_ids.append(eos_idx)\n",
       "\n",
       "    # Truncate if too long\n",
       "    if len(token_ids) > max_length:\n",
       "        # Keep cls, truncate middle, keep end\n",
       "        token_ids = token_ids[:1] + token_ids[1:max_length-1] + [eos_idx]\n",
       "\n",
       "    # Pad if needed\n",
       "    pad_length = max_length - len(token_ids)\n",
       "    if pad_length > 0:\n",
       "        token_ids = token_ids + [pad_idx] * pad_length\n",
       "\n",
       "    return torch.tensor(token_ids, dtype=torch.long)\n",
       "\n",
       "\n",
       "class DirectContrastiveDataset(Dataset):\n",
       "    \"\"\"Dataset that directly takes sequences and tokenizes them.\"\"\"\n",
       "\n",
       "    def __init__(\n",
       "        self,\n",
       "        cdr3_1_list,\n",
       "        antigen_1_list,\n",
       "        cdr3_2_list,\n",
       "        antigen_2_list,\n",
       "        labels,\n",
       "        max_length=384\n",
       "    ):\n",
       "        \"\"\"Initialize dataset with direct sequence data.\n",
       "        \n",
       "        Args:\n",
       "            cdr3_1_list: List of CDR3 sequences for first pair member\n",
       "            antigen_1_list: List of antigen sequences for first pair member\n",
       "            cdr3_2_list: List of CDR3 sequences for second pair member\n",
       "            antigen_2_list: List of antigen sequences for second pair member\n",
       "            labels: List of labels (1 for positive, 0 for negative)\n",
       "            max_length: Maximum sequence length\n",
       "        \"\"\"\n",
       "        self.cdr3_1 = cdr3_1_list\n",
       "        self.antigen_1 = antigen_1_list\n",
       "        self.cdr3_2 = cdr3_2_list\n",
       "        self.antigen_2 = antigen_2_list\n",
       "        self.labels = labels\n",
       "        self.max_length = max_length\n",
       "\n",
       "        # Define special tokens for ESM3\n",
       "        self.pad_idx = 1  # Padding token\n",
       "        self.cls_idx = 0  # Beginning of sequence\n",
       "        self.eos_idx = 2  # End of sequence\n",
       "        self.unk_idx = 3  # Unknown amino acid\n",
       "\n",
       "        # Standard amino acid mapping for ESM3\n",
       "        self.aa_to_idx = {\n",
       "            'A': 4, 'R': 5, 'N': 6, 'D': 7, 'C': 8, 'Q': 9, 'E': 10, 'G': 11,\n",
       "            'H': 12, 'I': 13, 'L': 14, 'K': 15, 'M': 16, 'F': 17, 'P': 18,\n",
       "            'S': 19, 'T': 20, 'W': 21, 'Y': 22, 'V': 23, 'B': 24, 'J': 25,\n",
       "            'O': 26, 'U': 27, 'X': 28, 'Z': 29, '.': 30, '-': 31, '|': 31\n",
       "        }\n",
       "\n",
       "        # Verify data length consistency\n",
       "        assert (\n",
       "            len(cdr3_1_list) == len(antigen_1_list) == len(cdr3_2_list) \n",
       "            == len(antigen_2_list) == len(labels)\n",
       "        )\n",
       "        print(f\"Created dataset with {len(labels)} pairs\")\n",
       "\n",
       "    def __len__(self):\n",
       "        \"\"\"Return the number of samples in the dataset.\"\"\"\n",
       "        return len(self.labels)\n",
       "\n",
       "    def tokenize_sequence(self, sequence):\n",
       "        \"\"\"Tokenize a protein sequence using ESM3's tokenization approach.\n",
       "        \n",
       "        Args:\n",
       "            sequence: Amino acid sequence\n",
       "\n",
       "        Returns:\n",
       "            List of token IDs\n",
       "        \"\"\"\n",
       "        # Start with <cls>\n",
       "        token_ids = [self.cls_idx]\n",
       "\n",
       "        # Tokenize sequence\n",
       "        for aa in sequence:\n",
       "            # Skip invalid characters\n",
       "            if aa in ' \\t\\n\\r':\n",
       "                continue\n",
       "            # Convert to uppercase and get token ID\n",
       "            token_ids.append(self.aa_to_idx.get(aa.upper(), self.unk_idx))\n",
       "\n",
       "        # Add end of sequence\n",
       "        token_ids.append(self.eos_idx)\n",
       "\n",
       "        # Truncate if too long\n",
       "        if len(token_ids) > self.max_length:\n",
       "            # Keep cls, truncate middle, keep end\n",
       "            token_ids = (\n",
       "                token_ids[:1] + token_ids[1:self.max_length-1] + [self.eos_idx]\n",
       "            )\n",
       "\n",
       "        # Pad if needed\n",
       "        pad_length = self.max_length - len(token_ids)\n",
       "        if pad_length > 0:\n",
       "            token_ids = token_ids + [self.pad_idx] * pad_length\n",
       "\n",
       "        return token_ids\n",
       "\n",
       "    def __getitem__(self, idx):\n",
       "        \"\"\"Get a sample from the dataset.\n",
       "        \n",
       "        Args:\n",
       "            idx: Index of the sample\n",
       "\n",
       "        Returns:\n",
       "            dict: Sample data\n",
       "        \"\"\"\n",
       "        # Get sequences for the pair\n",
       "        cdr3_1 = self.cdr3_1[idx]\n",
       "        antigen_1 = self.antigen_1[idx]\n",
       "        cdr3_2 = self.cdr3_2[idx]\n",
       "        antigen_2 = self.antigen_2[idx]\n",
       "\n",
       "        # Combine CDR3 and antigen for each pair\n",
       "        sequence1 = str(cdr3_1) + \"|\" + str(antigen_1)\n",
       "        sequence2 = str(cdr3_2) + \"|\" + str(antigen_2)\n",
       "\n",
       "        # Tokenize sequences\n",
       "        tokens1 = self.tokenize_sequence(sequence1)\n",
       "        tokens2 = self.tokenize_sequence(sequence2)\n",
       "\n",
       "        # Create attention masks (1 for tokens, 0 for padding)\n",
       "        attn_mask1 = [\n",
       "            1 if i != self.pad_idx else 0 for i in tokens1\n",
       "        ]\n",
       "        attn_mask2 = [\n",
       "            1 if i != self.pad_idx else 0 for i in tokens2\n",
       "        ]\n",
       "\n",
       "        # Get label\n",
       "        label = self.labels[idx]\n",
       "\n",
       "        return {\n",
       "            'tokens1': torch.tensor(tokens1, dtype=torch.long),\n",
       "            'attn_mask1': torch.tensor(attn_mask1, dtype=torch.long),\n",
       "            'tokens2': torch.tensor(tokens2, dtype=torch.long),\n",
       "            'attn_mask2': torch.tensor(attn_mask2, dtype=torch.long),\n",
       "            'label': torch.tensor(label, dtype=torch.float)\n",
       "        }"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 4. Data Loader Creation Functions\n",
       "We define a function to **create_test_dataloader** from a test pairs CSV and optionally match them to raw data. If needed, it falls back on **create_synthetic_dataset** for demonstration."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def create_test_dataloader(\n",
       "    test_pairs_path,\n",
       "    raw_data_path=None,\n",
       "    batch_size=32,\n",
       "    num_workers=2\n",
       "):\n",
       "    \"\"\"Create a test DataLoader with comprehensive ID matching strategies.\n",
       "\n",
       "    Args:\n",
       "        test_pairs_path: Path to CSV with test pairs\n",
       "        raw_data_path: Path to raw data CSV\n",
       "        batch_size: Batch size for DataLoader\n",
       "        num_workers: Number of workers for DataLoader\n",
       "\n",
       "    Returns:\n",
       "        DataLoader: Configured test data loader\n",
       "    \"\"\"\n",
       "    # Load test pairs\n",
       "    test_pairs = pd.read_csv(test_pairs_path)\n",
       "    print(f\"Loaded {len(test_pairs)} pairs from {test_pairs_path}\")\n",
       "\n",
       "    if 'ID1' in test_pairs.columns and 'ID2' in test_pairs.columns and raw_data_path:\n",
       "        # Load raw data\n",
       "        raw_data = pd.read_csv(raw_data_path)\n",
       "        print(f\"Loaded raw data with {len(raw_data)} rows from {raw_data_path}\")\n",
       "\n",
       "        # Define columns to use\n",
       "        id_column = 'ID_slide_Variant'\n",
       "        cdr3_col = 'CDR3'\n",
       "        antigen_col = 'antigen_sequence'\n",
       "\n",
       "        # Create multiple indices for the raw data\n",
       "        print(\"Creating multiple indices for raw data...\")\n",
       "        id_mappings = {\n",
       "            'full_id': {},                 # Full ID\n",
       "            'numeric_prefix': {},          # Numeric part before underscore\n",
       "            'digits_only': {},             # Just the digits from the ID\n",
       "            'substring': {},               # For substring matching\n",
       "            'first_digits': {},            # First N digits\n",
       "            'last_digits': {}              # Last N digits\n",
       "        }\n",
       "\n",
       "        for idx, row in raw_data.iterrows():\n",
       "            # Get the full ID\n",
       "            full_id = str(row[id_column])\n",
       "            id_mappings['full_id'][full_id] = row\n",
       "\n",
       "            # Get numeric prefix\n",
       "            if '_' in full_id:\n",
       "                prefix = full_id.split('_')[0]\n",
       "                id_mappings['numeric_prefix'][prefix] = row\n",
       "\n",
       "                # Store in digits-only map if the prefix is purely numeric\n",
       "                if prefix.isdigit():\n",
       "                    id_mappings['digits_only'][prefix] = row\n",
       "\n",
       "                    # Store first 5-7 digits for prefix matching\n",
       "                    for i in range(5, min(8, len(prefix) + 1)):\n",
       "                        if prefix[:i] not in id_mappings['first_digits']:\n",
       "                            id_mappings['first_digits'][prefix[:i]] = []\n",
       "                        id_mappings['first_digits'][prefix[:i]].append(row)\n",
       "\n",
       "                    # Store last 5-7 digits for suffix matching\n",
       "                    for i in range(5, min(8, len(prefix) + 1)):\n",
       "                        if prefix[-i:] not in id_mappings['last_digits']:\n",
       "                            id_mappings['last_digits'][prefix[-i:]] = []\n",
       "                        id_mappings['last_digits'][prefix[-i:]].append(row)\n",
       "\n",
       "        # Print stats about the mappings\n",
       "        for mapping_name, mapping in id_mappings.items():\n",
       "            if mapping_name in ['first_digits', 'last_digits']:\n",
       "                total = sum(len(rows) for rows in mapping.values())\n",
       "                print(\n",
       "                    f\"  {mapping_name}: {len(mapping)} unique patterns \"\n",
       "                    f\"with {total} total mappings\"\n",
       "                )\n",
       "            else:\n",
       "                print(f\"  {mapping_name}: {len(mapping)} unique entries\")\n",
       "\n",
       "        # Lists for dataset creation\n",
       "        cdr3_1_list = []\n",
       "        antigen_1_list = []\n",
       "        cdr3_2_list = []\n",
       "        antigen_2_list = []\n",
       "        labels = []\n",
       "\n",
       "        # Track matching results\n",
       "        match_stats = {\n",
       "            'pairs_processed': 0,\n",
       "            'matches_found': 0,\n",
       "            'match_types': {}\n",
       "        }\n",
       "\n",
       "        # Helper function to find a match for a single ID\n",
       "        def find_match_for_id(id_str):\n",
       "            \"\"\"Find a match for an ID using various strategies.\n",
       "            \n",
       "            Args:\n",
       "                id_str: ID string to match\n",
       "\n",
       "            Returns:\n",
       "                tuple: (matched_row, strategy_name) or (None, None)\n",
       "            \"\"\"\n",
       "            # Try different matching strategies in order of preference\n",
       "            match_strategies = [\n",
       "                (\n",
       "                    'exact_match',\n",
       "                    lambda id_str: id_mappings['digits_only'].get(id_str)\n",
       "                ),\n",
       "                (\n",
       "                    'first_digits',\n",
       "                    lambda id_str: id_mappings['first_digits'].get(id_str[:7])\n",
       "                ),\n",
       "                (\n",
       "                    'first_digits_short',\n",
       "                    lambda id_str: id_mappings['first_digits'].get(id_str[:6])\n",
       "                ),\n",
       "                (\n",
       "                    'first_digits_shorter',\n",
       "                    lambda id_str: id_mappings['first_digits'].get(id_str[:5])\n",
       "                ),\n",
       "                (\n",
       "                    'last_digits',\n",
       "                    lambda id_str: (\n",
       "                        id_mappings['last_digits'].get(id_str[-7:])\n",
       "                        if len(id_str) >= 7 else None\n",
       "                    )\n",
       "                ),\n",
       "                (\n",
       "                    'last_digits_short',\n",
       "                    lambda id_str: (\n",
       "                        id_mappings['last_digits'].get(id_str[-6:])\n",
       "                        if len(id_str) >= 6 else None\n",
       "                    )\n",
       "                ),\n",
       "                (\n",
       "                    'last_digits_shorter',\n",
       "                    lambda id_str: (\n",
       "                        id_mappings['last_digits'].get(id_str[-5:])\n",
       "                        if len(id_str) >= 5 else None\n",
       "                    )\n",
       "                )\n",
       "            ]\n",
       "\n",
       "            for strategy_name, strategy_func in match_strategies:\n",
       "                result = strategy_func(id_str)\n",
       "                if result is not None:\n",
       "                    # If we got a list of matches, use the first one\n",
       "                    if isinstance(result, list):\n",
       "                        if result:\n",
       "                            return result[0], strategy_name\n",
       "                    else:\n",
       "                        return result, strategy_name\n",
       "\n",
       "            # No match found\n",
       "            return None, None\n",
       "\n",
       "        # Process each pair\n",
       "        for idx, row in test_pairs.iterrows():\n",
       "            id1 = str(row['ID1'])\n",
       "            id2 = str(row['ID2'])\n",
       "\n",
       "            # Get label from pair_type\n",
       "            pair_type = row['pair_type'] if 'pair_type' in row else ''\n",
       "            label = 1.0 if pair_type.lower() == 'positive' else 0.0\n",
       "\n",
       "            # Find matches for both IDs\n",
       "            match1, match_type1 = find_match_for_id(id1)\n",
       "            match2, match_type2 = find_match_for_id(id2)\n",
       "\n",
       "            match_stats['pairs_processed'] += 1\n",
       "\n",
       "            # If both IDs match, add to dataset\n",
       "            if match1 is not None and match2 is not None:\n",
       "                # Extract sequences\n",
       "                cdr3_1 = match1[cdr3_col]\n",
       "                antigen_1 = match1[antigen_col]\n",
       "                cdr3_2 = match2[cdr3_col]\n",
       "                antigen_2 = match2[antigen_col]\n",
       "\n",
       "                # Add to dataset\n",
       "                cdr3_1_list.append(cdr3_1)\n",
       "                antigen_1_list.append(antigen_1)\n",
       "                cdr3_2_list.append(cdr3_2)\n",
       "                antigen_2_list.append(antigen_2)\n",
       "                labels.append(label)\n",
       "\n",
       "                # Update match statistics\n",
       "                match_stats['matches_found'] += 1\n",
       "                match_stats['match_types'][match_type1] = (\n",
       "                    match_stats['match_types'].get(match_type1, 0) + 1\n",
       "                )\n",
       "                match_stats['match_types'][match_type2] = (\n",
       "                    match_stats['match_types'].get(match_type2, 0) + 1\n",
       "                )\n",
       "\n",
       "                # Print sample matches for debugging (limit to first few)\n",
       "                if match_stats['matches_found'] <= 5:\n",
       "                    print(f\"\\nMatch {match_stats['matches_found']}:\")\n",
       "                    print(f\"  ID1: {id1} ({match_type1}) -> {match1[id_column]}\")\n",
       "                    print(f\"  ID2: {id2} ({match_type2}) -> {match2[id_column]}\")\n",
       "                    print(f\"  Label: {label} (from pair_type: {pair_type})\")\n",
       "                    print(f\"  CDR3: {cdr3_1[:15]}... & {cdr3_2[:15]}...\")\n",
       "                    print(f\"  Antigen: {antigen_1[:15]}... & {antigen_2[:15]}...\")\n",
       "\n",
       "            # Progress updates\n",
       "            if idx % 1000 == 0 and idx > 0:\n",
       "                print(\n",
       "                    f\"Processed {idx}/{len(test_pairs)} pairs, \"\n",
       "                    f\"found {match_stats['matches_found']} matches\"\n",
       "                )\n",
       "\n",
       "        # Print summary statistics\n",
       "        print(f\"\\nMatching summary:\")\n",
       "        print(f\"  Total pairs processed: {match_stats['pairs_processed']}\")\n",
       "        print(\n",
       "            f\"  Total matches found: {match_stats['matches_found']} \"\n",
       "            f\"({match_stats['matches_found']/match_stats['pairs_processed']*100:.2f}%)\"\n",
       "        )\n",
       "\n",
       "        # Print match type statistics\n",
       "        print(\"\\nMatch type statistics:\")\n",
       "        total_ids = 2 * match_stats['matches_found']  # Each match has 2 IDs\n",
       "        for match_type, count in match_stats['match_types'].items():\n",
       "            print(\n",
       "                f\"  {match_type}: {count} matches \"\n",
       "                f\"({count/total_ids*100:.2f}% of matched IDs)\"\n",
       "            )\n",
       "\n",
       "        # If not enough matches, use synthetic data\n",
       "        if match_stats['matches_found'] < 10:\n",
       "            print(\n",
       "                \"Insufficient matches for meaningful evaluation. \"\n",
       "                \"Using synthetic data instead.\"\n",
       "            )\n",
       "            return create_synthetic_dataset(batch_size, num_workers)\n",
       "\n",
       "        # Create dataset with matched pairs\n",
       "        print(f\"\\nCreating dataset with {len(labels)} matched pairs\")\n",
       "        test_dataset = DirectContrastiveDataset(\n",
       "            cdr3_1_list=cdr3_1_list,\n",
       "            antigen_1_list=antigen_1_list,\n",
       "            cdr3_2_list=cdr3_2_list,\n",
       "            antigen_2_list=antigen_2_list,\n",
       "            labels=labels\n",
       "        )\n",
       "    else:\n",
       "        print(\"Required columns not found or raw_data_path not provided.\")\n",
       "        return create_synthetic_dataset(batch_size, num_workers)\n",
       "\n",
       "    # Create DataLoader\n",
       "    test_loader = DataLoader(\n",
       "        test_dataset,\n",
       "        batch_size=batch_size,\n",
       "        shuffle=False,\n",
       "        num_workers=num_workers\n",
       "    )\n",
       "\n",
       "    return test_loader\n",
       "\n",
       "\n",
       "def create_synthetic_dataset(batch_size=32, num_workers=2):\n",
       "    \"\"\"Create a synthetic dataset for testing when real data cannot be loaded.\n",
       "    \n",
       "    Args:\n",
       "        batch_size: Batch size for DataLoader\n",
       "        num_workers: Number of workers for DataLoader\n",
       "\n",
       "    Returns:\n",
       "        DataLoader: DataLoader with synthetic test data\n",
       "    \"\"\"\n",
       "    print(\"Creating a synthetic dataset with diverse examples...\")\n",
       "\n",
       "    # Define several CDR3 and antigen sequences for variety\n",
       "    cdr3_sequences = [\n",
       "        \"CASSQDMTV\",\n",
       "        \"CASSFGQETQYF\",\n",
       "        \"CASSYEQYF\",\n",
       "        \"CASSPGQGAGELF\",\n",
       "        \"CSARQNRNYGYTF\"\n",
       "    ]\n",
       "\n",
       "    antigen_sequences = [\n",
       "        \"VPGFPTVRRALVPK\",\n",
       "        \"LLFGYPVYV\",\n",
       "        \"KLVALGINAVAQQANEES\",\n",
       "        \"KRWIILGLNKIVRMY\",\n",
       "        \"GILGFVFTLTV\"\n",
       "    ]\n",
       "\n",
       "    # Create datasets with controlled similarities\n",
       "    cdr3_1_list = []\n",
       "    antigen_1_list = []\n",
       "    cdr3_2_list = []\n",
       "    antigen_2_list = []\n",
       "    labels = []\n",
       "\n",
       "    # Create positive pairs (identical sequences)\n",
       "    for cdr3 in cdr3_sequences:\n",
       "        for antigen in antigen_sequences:\n",
       "            cdr3_1_list.append(cdr3)\n",
       "            antigen_1_list.append(antigen)\n",
       "            cdr3_2_list.append(cdr3)\n",
       "            antigen_2_list.append(antigen)\n",
       "            labels.append(1.0)\n",
       "\n",
       "    # Create negative pairs (different sequences)\n",
       "    for i, (cdr3_1, antigen_1) in enumerate(zip(cdr3_sequences, antigen_sequences)):\n",
       "        for j, (cdr3_2, antigen_2) in enumerate(zip(cdr3_sequences, antigen_sequences)):\n",
       "            if i != j:\n",
       "                cdr3_1_list.append(cdr3_1)\n",
       "                antigen_1_list.append(antigen_1)\n",
       "                cdr3_2_list.append(cdr3_2)\n",
       "                antigen_2_list.append(antigen_2)\n",
       "                labels.append(0.0)\n",
       "\n",
       "    # Create dataset\n",
       "    print(f\"Created synthetic dataset with {len(labels)} pairs\")\n",
       "    print(\n",
       "        f\"Positive pairs: {sum(labels)}, \"\n",
       "        f\"Negative pairs: {len(labels) - sum(labels)}\"\n",
       "    )\n",
       "\n",
       "    test_dataset = DirectContrastiveDataset(\n",
       "        cdr3_1_list=cdr3_1_list,\n",
       "        antigen_1_list=antigen_1_list,\n",
       "        cdr3_2_list=cdr3_2_list,\n",
       "        antigen_2_list=antigen_2_list,\n",
       "        labels=labels\n",
       "    )\n",
       "\n",
       "    # Create and return the DataLoader\n",
       "    test_loader = DataLoader(\n",
       "        test_dataset,\n",
       "        batch_size=batch_size,\n",
       "        shuffle=False,\n",
       "        num_workers=num_workers\n",
       "    )\n",
       "\n",
       "    return test_loader\n"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 5. Inference and Metric Computation Functions\n",
       "Utility functions to:\n",
       "- **run_inference_on_test_set**: Evaluate a model on a test set.\n",
       "- **inference_on_sequence_pair**: Run inference on a single pair.\n",
       "Also includes metric computation and optional plotting of results."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def run_inference_on_test_set(\n",
       "    model,\n",
       "    test_loader,\n",
       "    device=None,\n",
       "    similarity_threshold=0.7\n",
       "):\n",
       "    \"\"\"Run inference on a test set and compute metrics with improved threshold.\n",
       "\n",
       "    Args:\n",
       "        model: The loaded contrastive model\n",
       "        test_loader: DataLoader with test data\n",
       "        device: Computation device\n",
       "        similarity_threshold: Threshold for binary classification\n",
       "\n",
       "    Returns:\n",
       "        tuple: (metrics_dict, similarities, labels)\n",
       "    \"\"\"\n",
       "    device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
       "\n",
       "    # Lists to store results\n",
       "    all_similarities = []\n",
       "    all_labels = []\n",
       "\n",
       "    # Inference loop with error handling\n",
       "    with torch.no_grad():\n",
       "        for batch_idx, batch in enumerate(test_loader):\n",
       "            try:\n",
       "                # Move data to device\n",
       "                tokens1 = batch['tokens1'].to(device)\n",
       "                tokens2 = batch['tokens2'].to(device)\n",
       "                labels = batch['label'].cpu().numpy()\n",
       "\n",
       "                # Get embeddings from model\n",
       "                z1 = model(tokens1)\n",
       "                z2 = model(tokens2)\n",
       "\n",
       "                # Use model's similarity function if available, otherwise use default\n",
       "                if hasattr(model, 'compute_similarity'):\n",
       "                    similarities = model.compute_similarity(z1, z2).cpu().numpy()\n",
       "                else:\n",
       "                    # Default cosine similarity\n",
       "                    similarities = F.cosine_similarity(z1, z2, dim=1).cpu().numpy()\n",
       "\n",
       "                # Store results\n",
       "                all_similarities.extend(similarities)\n",
       "                all_labels.extend(labels)\n",
       "\n",
       "                # Progress update\n",
       "                if batch_idx % 5 == 0:\n",
       "                    print(f\"Processed {batch_idx}/{len(test_loader)} batches\")\n",
       "\n",
       "            except Exception as e:\n",
       "                print(f\"Error processing batch {batch_idx}: {e}\")\n",
       "                continue\n",
       "\n",
       "    # Convert to numpy arrays\n",
       "    all_similarities = np.array(all_similarities)\n",
       "    all_labels = np.array(all_labels)\n",
       "\n",
       "    # Check if we have any valid results\n",
       "    if len(all_similarities) == 0 or len(all_labels) == 0:\n",
       "        print(\"No valid results to evaluate!\")\n",
       "        return {\n",
       "            'roc_auc': 0.0,\n",
       "            'pr_auc': 0.0,\n",
       "            'accuracy': 0.0,\n",
       "            'precision': 0.0,\n",
       "            'recall': 0.0,\n",
       "            'f1': 0.0\n",
       "        }, all_similarities, all_labels\n",
       "\n",
       "    # Compute metrics\n",
       "    metrics = {}\n",
       "\n",
       "    # ROC AUC\n",
       "    try:\n",
       "        metrics['roc_auc'] = roc_auc_score(all_labels, all_similarities)\n",
       "    except Exception as e:\n",
       "        print(f\"Error computing ROC AUC: {e}\")\n",
       "        metrics['roc_auc'] = 0.0\n",
       "\n",
       "    # PR AUC\n",
       "    try:\n",
       "        precision, recall, _ = precision_recall_curve(all_labels, all_similarities)\n",
       "        metrics['pr_auc'] = auc(recall, precision)\n",
       "    except Exception as e:\n",
       "        print(f\"Error computing PR AUC: {e}\")\n",
       "        metrics['pr_auc'] = 0.0\n",
       "\n",
       "    # Apply threshold and compute accuracy\n",
       "    binary_preds = (all_similarities >= similarity_threshold).astype(int)\n",
       "    metrics['accuracy'] = np.mean(binary_preds == all_labels)\n",
       "\n",
       "    # True positives, false positives, etc.\n",
       "    tp = np.sum((binary_preds == 1) & (all_labels == 1))\n",
       "    tn = np.sum((binary_preds == 0) & (all_labels == 0))\n",
       "    fp = np.sum((binary_preds == 1) & (all_labels == 0))\n",
       "    fn = np.sum((binary_preds == 0) & (all_labels == 1))\n",
       "\n",
       "    # Precision and recall\n",
       "    metrics['precision'] = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
       "    metrics['recall'] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
       "    metrics['f1'] = (\n",
       "        2 * (metrics['precision'] * metrics['recall']) \n",
       "        / (metrics['precision'] + metrics['recall'])\n",
       "        if (metrics['precision'] + metrics['recall']) > 0 else 0\n",
       "    )\n",
       "\n",
       "    # Calculate optimal threshold\n",
       "    try:\n",
       "        precision, recall, thresholds = precision_recall_curve(\n",
       "            all_labels, all_similarities\n",
       "        )\n",
       "        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
       "        optimal_idx = np.argmax(f1_scores)\n",
       "        optimal_threshold = (\n",
       "            thresholds[optimal_idx] \n",
       "            if optimal_idx < len(thresholds) else thresholds[-1]\n",
       "        )\n",
       "        metrics['optimal_threshold'] = float(optimal_threshold)\n",
       "        metrics['optimal_f1'] = float(f1_scores[optimal_idx])\n",
       "\n",
       "        # Add metrics with optimal threshold\n",
       "        binary_preds_optimal = (all_similarities >= optimal_threshold).astype(int)\n",
       "        metrics['accuracy_optimal'] = float(\n",
       "            np.mean(binary_preds_optimal == all_labels)\n",
       "        )\n",
       "\n",
       "        tp_opt = np.sum((binary_preds_optimal == 1) & (all_labels == 1))\n",
       "        fp_opt = np.sum((binary_preds_optimal == 1) & (all_labels == 0))\n",
       "        fn_opt = np.sum((binary_preds_optimal == 0) & (all_labels == 1))\n",
       "\n",
       "        metrics['precision_optimal'] = float(\n",
       "            tp_opt / (tp_opt + fp_opt) if (tp_opt + fp_opt) > 0 else 0\n",
       "        )\n",
       "        metrics['recall_optimal'] = float(\n",
       "            tp_opt / (tp_opt + fn_opt) if (tp_opt + fn_opt) > 0 else 0\n",
       "        )\n",
       "    except Exception as e:\n",
       "        print(f\"Error computing optimal threshold: {e}\")\n",
       "\n",
       "    # Calculate similarity statistics\n",
       "    metrics['min_similarity'] = float(np.min(all_similarities))\n",
       "    metrics['max_similarity'] = float(np.max(all_similarities))\n",
       "    metrics['mean_similarity'] = float(np.mean(all_similarities))\n",
       "    metrics['mean_similarity_positive'] = float(\n",
       "        np.mean(all_similarities[all_labels == 1])\n",
       "    )\n",
       "    metrics['mean_similarity_negative'] = float(\n",
       "        np.mean(all_similarities[all_labels == 0])\n",
       "    )\n",
       "\n",
       "    return metrics, all_similarities, all_labels\n",
       "\n",
       "\n",
       "def inference_on_sequence_pair(\n",
       "    model, cdr3_1, antigen_1, cdr3_2, antigen_2, device=None\n",
       "):\n",
       "    \"\"\"Run inference on a single pair of sequences.\n",
       "    \n",
       "    Args:\n",
       "        model: The loaded contrastive model\n",
       "        cdr3_1 (str): First CDR3 sequence\n",
       "        antigen_1 (str): First antigen sequence\n",
       "        cdr3_2 (str): Second CDR3 sequence\n",
       "        antigen_2 (str): Second antigen sequence\n",
       "        device: Computation device\n",
       "    \n",
       "    Returns:\n",
       "        float: Similarity score between the sequences\n",
       "    \"\"\"\n",
       "    device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
       "\n",
       "    # Combine sequences with separator\n",
       "    sequence1 = str(cdr3_1) + \"|\" + str(antigen_1)\n",
       "    sequence2 = str(cdr3_2) + \"|\" + str(antigen_2)\n",
       "\n",
       "    # Tokenize sequences\n",
       "    tokens1 = tokenize_sequence(sequence1).unsqueeze(0).to(device)\n",
       "    tokens2 = tokenize_sequence(sequence2).unsqueeze(0).to(device)\n",
       "\n",
       "    # Get embeddings\n",
       "    with torch.no_grad():\n",
       "        z1 = model(tokens1)\n",
       "        z2 = model(tokens2)\n",
       "\n",
       "        # Use model's similarity function if available, otherwise use default\n",
       "        if hasattr(model, 'compute_similarity'):\n",
       "            similarity = model.compute_similarity(z1, z2).item()\n",
       "        else:\n",
       "            # Default cosine similarity\n",
       "            similarity = F.cosine_similarity(z1, z2, dim=1).item()\n",
       "\n",
       "    return similarity\n"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 6. Main Inference Function\n",
       "Finally, we define our main entry point that:\n",
       "- Loads ESM3 and optionally a finetuned checkpoint.\n",
       "- Wraps the backbone in our enhanced model.\n",
       "- Loads the contrastive head weights.\n",
       "- Creates a test dataloader.\n",
       "- Runs inference to compute metrics.\n",
       "- Generates sample plots (similarity distribution, ROC)."
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def main_inference():\n",
       "    \"\"\"Main function to run inference on a test set.\"\"\"\n",
       "    # Paths\n",
       "    model_weights_path = '/home/jupyter/checkpoints/model_epoch_7.pt'\n",
       "    esm3_checkpoint_path = '/home/jupyter/oracle/esm3_finetuned_antibody/checkpoint_epoch_1.pt'\n",
       "    test_pairs_path = '/home/jupyter/DATA/hyperbind_train/synthetic/test_pairs.csv'\n",
       "    raw_data_path = '/home/jupyter/DATA/hyperbind_train/synthetic/test.csv'\n",
       "\n",
       "    # Set device\n",
       "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
       "    print(f\"Using device: {device}\")\n",
       "\n",
       "    try:\n",
       "        # Load ESM3 model\n",
       "        print(\"Loading ESM3 backbone...\")\n",
       "        from esm.pretrained import ESM3_sm_open_v0\n",
       "        esm3_model = ESM3_sm_open_v0()\n",
       "        print(\"ESM3 model instantiated\")\n",
       "\n",
       "        # Load finetuned weights for the backbone\n",
       "        esm3_ckpt = torch.load(esm3_checkpoint_path, map_location=device)\n",
       "        esm3_state_dict = esm3_ckpt.get('model_state_dict', esm3_ckpt)\n",
       "        esm3_model.load_state_dict(esm3_state_dict, strict=False)\n",
       "        print(\"Loaded finetuned ESM3 backbone weights\")\n",
       "\n",
       "        # Wrap backbone in contrastive model\n",
       "        model = EnhancedContrastiveModel(\n",
       "            esm3_model, freeze_backbone=False, temperature=0.15\n",
       "        )\n",
       "        print(\"Enhanced contrastive model created\")\n",
       "\n",
       "        # Load contrastive head weights\n",
       "        print(f\"Loading contrastive head weights from {model_weights_path}\")\n",
       "        checkpoint = torch.load(model_weights_path, map_location=device)\n",
       "        state_dict = checkpoint.get('model_state_dict', checkpoint)\n",
       "        model.load_state_dict(state_dict, strict=False)\n",
       "        print(\"Contrastive head weights loaded (with some mismatches allowed)\")\n",
       "\n",
       "        # Print summary of loaded keys\n",
       "        missing_keys = set(k for k, _ in model.named_parameters()) - set(state_dict.keys())\n",
       "        unexpected_keys = set(state_dict.keys()) - set(k for k, _ in model.named_parameters())\n",
       "        print(f\"Missing keys: {len(missing_keys)}\")\n",
       "        print(f\"Unexpected keys: {len(unexpected_keys)}\")\n",
       "        print(f\"Successfully loaded keys: {len(state_dict) - len(unexpected_keys)}\")\n",
       "\n",
       "        # Prepare model for inference\n",
       "        model.to(device)\n",
       "        model.eval()\n",
       "        print(\"Model moved to device and set to eval mode\")\n",
       "\n",
       "        # Load test data\n",
       "        test_loader = create_test_dataloader(\n",
       "            test_pairs_path=test_pairs_path,\n",
       "            raw_data_path=raw_data_path,\n",
       "            batch_size=16\n",
       "        )\n",
       "        print(f\"Test loader created with {len(test_loader.dataset)} samples\")\n",
       "\n",
       "        # Run inference\n",
       "        print(\"Running inference on test set...\")\n",
       "        metrics, all_similarities, all_labels = run_inference_on_test_set(\n",
       "            model, test_loader, device=device, similarity_threshold=0.7\n",
       "        )\n",
       "\n",
       "        # Print evaluation metrics\n",
       "        print(\"\\nTest Set Metrics:\")\n",
       "        for metric_name, metric_value in metrics.items():\n",
       "            if isinstance(metric_value, float):\n",
       "                print(f\"{metric_name}: {metric_value:.4f}\")\n",
       "            else:\n",
       "                print(f\"{metric_name}: {metric_value}\")\n",
       "\n",
       "        # Plot similarity and ROC\n",
       "        try:\n",
       "            plt.figure(figsize=(10, 6))\n",
       "            plt.hist(\n",
       "                all_similarities[all_labels==1], \n",
       "                bins=50, alpha=0.5, label='Positive Pairs'\n",
       "            )\n",
       "            plt.hist(\n",
       "                all_similarities[all_labels==0], \n",
       "                bins=50, alpha=0.5, label='Negative Pairs'\n",
       "            )\n",
       "            plt.xlabel('Similarity Score')\n",
       "            plt.ylabel('Frequency')\n",
       "            plt.legend()\n",
       "            plt.title('Distribution of Similarity Scores')\n",
       "            plt.savefig('/home/jupyter/similarity_distribution.png')\n",
       "            print(\"Saved similarity distribution plot to /home/jupyter/similarity_distribution.png\")\n",
       "\n",
       "            fpr, tpr, _ = roc_curve(all_labels, all_similarities)\n",
       "            plt.figure(figsize=(10, 6))\n",
       "            plt.plot(\n",
       "                fpr, tpr, label=f'ROC Curve (AUC = {metrics[\"roc_auc\"]:.4f})'\n",
       "            )\n",
       "            plt.plot([0, 1], [0, 1], 'k--')\n",
       "            plt.xlabel('False Positive Rate')\n",
       "            plt.ylabel('True Positive Rate')\n",
       "            plt.title('ROC Curve')\n",
       "            plt.legend()\n",
       "            plt.savefig('/home/jupyter/roc_curve.png')\n",
       "            print(\"Saved ROC curve plot to /home/jupyter/roc_curve.png\")\n",
       "        except Exception as e:\n",
       "            print(f\"Plotting error: {e}\")\n",
       "\n",
       "        # Sample inference\n",
       "        print(\"\\nRunning sample inference:\")\n",
       "        cdr3_1 = \"CASSQDMTV\"\n",
       "        antigen_1 = \"VPGFPTVRRALVPK\"\n",
       "        cdr3_2 = \"CASSQDMTV\"\n",
       "        antigen_2 = \"VPGFPTVRRALVPK\"\n",
       "        similarity = inference_on_sequence_pair(\n",
       "            model, cdr3_1, antigen_1, cdr3_2, antigen_2, device=device\n",
       "        )\n",
       "        print(f\"Similarity between same sequences: {similarity:.4f}\")\n",
       "\n",
       "        cdr3_2 = \"CASSFGQETQYF\"\n",
       "        antigen_2 = \"LLFGYPVYV\"\n",
       "        similarity = inference_on_sequence_pair(\n",
       "            model, cdr3_1, antigen_1, cdr3_2, antigen_2, device=device\n",
       "        )\n",
       "        print(f\"Similarity between different sequences: {similarity:.4f}\")\n",
       "\n",
       "        # Save matched dataset\n",
       "        try:\n",
       "            matched_data = {\n",
       "                'cdr3_1': test_loader.dataset.cdr3_1,\n",
       "                'antigen_1': test_loader.dataset.antigen_1,\n",
       "                'cdr3_2': test_loader.dataset.cdr3_2,\n",
       "                'antigen_2': test_loader.dataset.antigen_2,\n",
       "                'label': test_loader.dataset.labels\n",
       "            }\n",
       "            matched_df = pd.DataFrame(matched_data)\n",
       "            matched_df.to_csv(\n",
       "                '/home/jupyter/DATA/hyperbind_train/matched_test_pairs.csv', \n",
       "                index=False\n",
       "            )\n",
       "            print(\n",
       "                \"Saved matched dataset to \"\n",
       "                \"/home/jupyter/DATA/hyperbind_train/matched_test_pairs.csv\"\n",
       "            )\n",
       "        except Exception as e:\n",
       "            print(f\"Could not save matched dataset: {e}\")\n",
       "\n",
       "        return model, metrics, all_similarities, all_labels\n",
       "\n",
       "    except Exception as e:\n",
       "        print(f\"Critical error in main_inference: {e}\")\n",
       "        import traceback\n",
       "        traceback.print_exc()\n",
       "        return None, None, None, None\n",
       "\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    main_inference()\n"
      ],
      "execution_count": null,
      "outputs": []
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "name": "python"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 5
   }
   
