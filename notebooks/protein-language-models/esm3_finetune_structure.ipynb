{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88d03c5c-adc8-4705-a3f3-d95b3cec5c63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Import necessary functions and classes from the codebase\n",
    "from esm.sdk.api import ESMProtein\n",
    "from esm.utils.structure.protein_chain import ProteinChain\n",
    "from esm.utils.encoding import tokenize_structure  # to convert raw structure to tokens/tensors\n",
    "from esm.tokenization.structure_tokenizer import StructureTokenizer\n",
    "from esm.models.vqvae import StructureTokenEncoder\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join('..','..', 'scripts','data_ingestion','antibody_structure_ingestion')))\n",
    "from pdb2esm import read_monomer_structure, read_multimer_structure, detect_and_process_structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613be047-2b92-4822-89fc-5fe181c5d6de",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### start by loading in ESMProteins from train directory...\n",
    "\n",
    "*The resulting object is not directly passed into model, but useful for validating the Encoder*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dd4be2-76ef-40ef-98f4-0b39fa561e37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Directory where training PDB files are stored\n",
    "train_directory = '/home/jupyter/DATA/hyperbind_train/sabdab/all_structures/train-test-split/'\n",
    "\n",
    "# List all files ending with \"_train.pdb\" in the directory\n",
    "train_pdb_files = [\n",
    "    os.path.join(train_directory, f)\n",
    "    for f in os.listdir(train_directory) if f.endswith(\"_train.pdb\")\n",
    "]\n",
    "\n",
    "# Process each file into an ESMProtein object\n",
    "esm_protein_list = []\n",
    "for pdb_path in train_pdb_files:\n",
    "    protein = detect_and_process_structure(pdb_path)\n",
    "    if protein is None:\n",
    "        print(f\"Warning: Failed to process {pdb_path}\")\n",
    "        continue\n",
    "    esm_protein_list.append(protein)\n",
    "\n",
    "# uncomment if you wish\n",
    "# print(f\"Loaded {len(esm_protein_list)} ESMProtein objects from training files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d5c43a-2a3e-4102-801a-cd2a1cc1644b",
   "metadata": {},
   "source": [
    "# Instantiate ESM3 Tokenizer + Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf8c188-75be-4aa7-bee2-a755d750d666",
   "metadata": {},
   "source": [
    "##### Instantiate the structure tokenizer and encoder. We will use pretrained parameters that match the setup native to ESM3-OS, but the untrained encoder is also provided below with a breakdown for our learning.\n",
    "\n",
    "The `structure_encoder` function shows that the pre‐training setup for the structure encoder. It used a d_model of 1024, 1 attention head (with additional parameters like v_heads, n_layers, etc.), a d_out of 128, and a codebook size of 4096. You can find this exact instantiation in the codebase (see esm/pretrained.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf38ee2-ffe4-4f41-91ef-cf436e1389c8",
   "metadata": {},
   "source": [
    "**The class definitions in esm/models/vqvae.py also reveal how parameters like d_model, n_heads, d_out, and n_codes are used to set up the structure token encoder, illustrating the design choices for handling protein structure. Read the notes below to learn.**\n",
    "\n",
    "•d_model:\n",
    "This is the dimensionality of the model’s hidden representations. In transformer architectures, each token (or residue, in the case of protein models) is represented by a vector of length d_model. For example, if d_model is 1024, every residue is encoded as a 1024-dimensional vector. This size is a design choice that balances model capacity with computational cost.\n",
    "\n",
    "•n_heads:\n",
    "In multi-head attention, n_heads determines how many separate attention “heads” the model uses. Each head performs its own attention calculation in a lower-dimensional subspace (i.e. d_model divided by n_heads) and the results are combined. More heads allow the model to capture different aspects or relationships within the data. In some configurations for the structure branch (as seen in esm/pretrained.py), you might see n_heads set to 1, meaning all attention is computed in a single head.\n",
    "\n",
    "•v_heads:\n",
    "The term “v_heads” stands for “voting heads” and is specific to parts of the architecture designed for geometric reasoning or structure prediction. Instead of (or in addition to) standard attention, these heads help aggregate structural information—effectively “voting” on spatial or geometric features needed to predict 3D structure. For instance, in the pretraining code for the structure encoder, v_heads might be set to 128, indicating that 128 separate voting mechanisms are used to capture geometric nuances.\n",
    "\n",
    "•n_layers:\n",
    "This parameter indicates the number of layers (or blocks) in the network. In a transformer, each layer typically consists of an attention mechanism followed by a feed-forward network. More layers usually allow the model to capture increasingly complex patterns.\n",
    "\n",
    "•d_out:\n",
    "In contexts like the structure token encoder, d_out is the dimension of the output projections. After processing with the transformer layers, the model might project the hidden representation to a lower-dimensional space (e.g. for predicting structure tokens or coordinates) where d_out defines that size.\n",
    "\n",
    "•n_codes:\n",
    "This parameter is used in models employing a vector quantization (VQ) approach (as in the VQ-VAE modules). n_codes is the number of discrete codes (or “entries”) in the codebook. During training, continuous latent representations are mapped to one of these codes, which helps in regularizing the model and capturing discrete structure in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a86b6a-23ee-47f8-b47d-8c7023f25c84",
   "metadata": {},
   "source": [
    "*Can we modify the encoder for antibodies?*\n",
    "\n",
    "1.\tPretrained Architecture Compatibility:\n",
    "The original ESM3 model’s configuration (with parameters like d_model, n_heads, v_heads, n_layers, etc.) was carefully tuned during pretraining. If you change these hyperparameters, you won’t be able to directly load the pretrained weights because the architecture will no longer match. In other words, modifying these settings without re‐training from scratch could “mess up” the model or lead to unpredictable behavior.\n",
    "\n",
    "2.\tTailoring for Antibody Structures:\n",
    "Antibodies, and especially their CDRH3 loops, often require capturing very fine, local structural details. One strategy is not necessarily to change the global architecture but rather to add specialized modules or adjust the attention mechanisms to focus more on local interactions in these regions. For example, you might incorporate a local refinement layer or tweak the geometric attention mechanism. That way, you retain the benefits of the pretrained ESM3 while enhancing its ability to model antibody-specific features.\n",
    "\n",
    "3.\tTrade-offs and Training Strategy:\n",
    "If you opt for a custom configuration that significantly changes parameters (like increasing the resolution for local regions by modifying d_model or n_heads), you will need to train the model (or at least the modified parts) from scratch or with careful fine-tuning. This can be resource intensive and might require a larger dataset that includes plenty of antibody examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8716911-8be4-4811-8ce3-400c698d7be7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "structure_tokenizer = StructureTokenizer()\n",
    "structure_encoder = StructureTokenEncoder(\n",
    "    d_model=1024, \n",
    "    n_heads=1, \n",
    "    v_heads=128, \n",
    "    n_layers=2, \n",
    "    d_out=128, \n",
    "    n_codes=4096\n",
    ").train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60a8593-86ff-4e21-bf41-96e7c7ff5fde",
   "metadata": {},
   "source": [
    "*use the pretrained encoder instead and finetune on this.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "582bc0bc-19f5-4de0-8298-37250e9c26df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructureTokenEncoder(\n",
       "  (transformer): GeometricEncoderStack(\n",
       "    (blocks): ModuleList(\n",
       "      (0-1): 2 x UnifiedTransformerBlock(\n",
       "        (geom_attn): GeometricReasoningOriginalImpl(\n",
       "          (s_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=1920, bias=True)\n",
       "          (out_proj): Linear(in_features=384, out_features=1024, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=1024, out_features=8192, bias=True)\n",
       "          (2): SwiGLU()\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pre_vq_proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "  (codebook): EMACodebook()\n",
       "  (relative_positional_embedding): RelativePositionEmbedding(\n",
       "    (embedding): Embedding(66, 1024)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from esm.pretrained import ESM3_structure_encoder_v0\n",
    "structure_tokenizer = StructureTokenizer()\n",
    "encoder = ESM3_structure_encoder_v0(device=\"cuda\")  # or \"cpu\"\n",
    "encoder.train()  # if you need to fine-tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2771b2de-5f72-462f-b5c9-c2414af5138a",
   "metadata": {},
   "source": [
    "### validate the encoder works on ESMProtein data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "511e4f59-4e73-4643-8ed0-04032168d5c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/esm/models/vqvae.py:286: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast(enabled=False):  # type: ignore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded coordinates shape: torch.Size([226, 37, 3])\n",
      "PLDDT shape: torch.Size([226])\n",
      "Structure tokens: tensor([4098,  144,   53, 3314, 1650, 1582, 3484, 3599, 2168, 3245, 3670, 2834,\n",
      "        2543, 1854, 2982, 3563, 1486, 2800, 1884,  597, 3748, 3881, 2952,  672,\n",
      "        2420,  965, 3564,  897, 1585,  620, 1067,  156, 3304,  821, 2165, 2084,\n",
      "        3075,  534, 3881,  747, 2227, 1600,  728, 3697, 3245, 1739, 2567,  539,\n",
      "        2517,  795, 3054, 4063, 1582,  976,  318, 3554, 2552, 3619, 2640,  104,\n",
      "        1429, 1842, 2938, 3915, 3372,   38, 1925, 2571,  747, 3748, 2408,  755,\n",
      "        2372, 1475, 3300, 3146, 3750, 3867, 2534,  618,  178, 1568, 3090, 3831,\n",
      "        2685, 2633, 1272,  280, 4028, 3462,  355, 1020,  554,  281, 3889, 2087,\n",
      "        3889,  421,  471, 1310, 1117, 1579,  621,  988, 3189, 3589, 2904, 3034,\n",
      "        1313, 3558, 2787, 3052, 1514, 2708, 4051, 1095, 1054, 2246, 1697, 3152,\n",
      "        1143, 2791,  121, 1207,   58,    3, 3296, 1918, 2408, 3912, 1419, 2953,\n",
      "        3366, 3563, 1413, 2502, 3572,  972, 2645, 3881, 2904,   90, 1378, 2389,\n",
      "        3248,  671, 2644, 1541, 3392, 2530, 2294, 2084, 2165, 2323,  614,  785,\n",
      "         991, 1600, 3725, 1025, 2998,  452, 3530, 2033, 1183, 2512, 2985, 2192,\n",
      "         363, 1427, 1385, 4013, 1392, 3927, 1669, 2977, 1316,  124, 1264, 3261,\n",
      "         719, 2894, 3538, 2485,  192, 2698, 3299,  377, 2895,  967, 2534,  309,\n",
      "        2240,  895, 2107,  114,  429, 2353, 1888,  739,    0, 1050,  554, 3600,\n",
      "        1572, 1111, 1572, 3560,  390, 1314, 2988, 3421,  783, 1410, 1581, 2062,\n",
      "        3409, 3558, 2787, 1447, 2503, 1777,  823, 1589, 1973, 4097],\n",
      "       device='cuda:0')\n",
      "Encoded coordinates shape: torch.Size([124, 37, 3])\n",
      "PLDDT shape: torch.Size([124])\n",
      "Structure tokens: tensor([4098,  145, 1278, 3329,  303, 2721, 1892, 1552, 1343, 3020, 3245,  854,\n",
      "        1548, 3366, 3717, 3621, 3756, 3572,  529, 2339,  836, 2952, 3745,  637,\n",
      "        1352, 2761, 3111, 2204,  620, 1737,  845, 4012,  740, 1199,   91, 3445,\n",
      "        3568, 3881, 1189,   11,  335, 1083, 4055, 3847, 2376,   99, 3333, 3864,\n",
      "        2632, 1226, 2890, 4013,  798, 2484, 2067,  116, 2873, 1226, 3058,  378,\n",
      "        1791,  182, 2032, 3977, 1658,  546, 1981, 3491, 1413, 2326, 2607,  782,\n",
      "        1339, 2547, 1012, 2151, 1525,  816, 2534,  618, 1632, 1820, 4032, 2701,\n",
      "         720, 1076, 2852, 2332, 3450, 2848, 2802, 4045, 1266, 3335, 2313, 3284,\n",
      "        3889, 2749, 1092,  254, 2311, 3778,   13, 3591, 1949, 3585, 2137,  824,\n",
      "        3109, 2712, 4081, 1859, 1257,  360,  966, 1344, 1543, 1006, 1655,  177,\n",
      "        4051, 1413, 3426, 4097], device='cuda:0')\n",
      "Encoded coordinates shape: torch.Size([233, 37, 3])\n",
      "PLDDT shape: torch.Size([233])\n",
      "Structure tokens: tensor([4098,  699, 3165, 3314, 3128, 1367, 3484, 2276, 2168,  313,  962, 1420,\n",
      "        3549, 3819, 3030,  546, 1272,  742, 1319, 1244, 2645, 3640, 1906,  473,\n",
      "        3054,  965,  640, 1275, 3111,  705, 3628, 1888, 4003,  821, 3686, 1443,\n",
      "        3408, 2925, 1717, 3483,  468, 1600,  728, 2210, 3245,  306, 2567, 3716,\n",
      "        2104, 2437, 3054, 1466, 1868,  425, 3983, 2204, 1351,  429, 3074,  392,\n",
      "        1546, 1847,  413,  639, 1219, 3999,   38, 2653,  404,  566, 1406, 1319,\n",
      "        2420, 4084, 1381,  987, 4055, 3750, 3536, 2534,  618,  844, 1459, 2175,\n",
      "        1888, 1810,  782, 3370, 2336, 2497, 3462,  355, 1050,   77, 1415, 3889,\n",
      "        3995,   91, 3481, 2329, 1363, 3336,  322, 2515,  642, 2435, 2448, 1422,\n",
      "         603, 2898, 3872,  312, 3220, 3663, 3558, 2787, 3376, 3399,  470, 1279,\n",
      "        1095, 1718, 2246, 1697, 3582, 3234, 3329, 1726, 1207, 3378, 1241,  476,\n",
      "        1623,  177, 3061, 3691, 2105, 2499, 1262, 2545,   56,  279, 1757, 2645,\n",
      "        3640, 2172, 1824,  178, 1397,  448, 2865,  697, 1541,  908, 2737,  653,\n",
      "        1443, 2730, 2313,  614,  785,  991, 1600, 3726, 1025, 2998, 3994, 2615,\n",
      "        1493, 3189,  981, 2985, 2192,  363, 1427, 1339, 2280,  654, 1235, 3927,\n",
      "        2567, 2859,  808, 1264, 2571, 1179, 2894, 1983, 1712, 2964, 1117, 3258,\n",
      "         461, 2895,  967, 1527,  309, 2240,  985, 2107, 1388,  429, 1600, 1888,\n",
      "        1246, 1190, 1050,  554, 3600, 2313, 3867, 2313, 2494, 2479, 3083, 2520,\n",
      "        1414, 1845, 2268, 1023, 2102, 3835, 3034, 1849, 3558, 2787, 1447, 1813,\n",
      "        1973,  823, 1924,  246, 4097], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/esm/models/vqvae.py:286: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast(enabled=False):  # type: ignore\n"
     ]
    }
   ],
   "source": [
    "# Set the encoder to evaluation mode if you are only testing the encoding.\n",
    "# (If you are fine-tuning, you'll later call .train())\n",
    "# structure_encoder.eval()\n",
    "encoder.eval()\n",
    "\n",
    "for protein in esm_protein_list[0:3]:\n",
    "    # Now, use the tokenize_structure utility to encode the protein.\n",
    "    # The function expects the raw coordinates, the encoder, the tokenizer,\n",
    "    # and the reference sequence (from the protein) as inputs.\n",
    "    encoded_coords, plddt, structure_tokens = tokenize_structure(\n",
    "        protein.coordinates,\n",
    "        encoder,\n",
    "        structure_tokenizer,\n",
    "        reference_sequence=protein.sequence,\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "\n",
    "    # Print out shapes and tokens to verify the encoding.\n",
    "    print(\"Encoded coordinates shape:\", encoded_coords.shape)\n",
    "    print(\"PLDDT shape:\", plddt.shape)\n",
    "    print(\"Structure tokens:\", structure_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59eec3ca-0608-4197-985d-faa4d59efd3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_protein(protein: ESMProtein):\n",
    "    \"\"\"\n",
    "    Encodes an ESMProtein object into tensors suitable for structure fine-tuning.\n",
    "    This uses the tokenize_structure function from the codebase to generate:\n",
    "      - encoded backbone coordinates input (processed coordinates)\n",
    "      - additional tokens if needed (here we return only the encoded coordinates).\n",
    "    \n",
    "    Assumes protein.coordinates holds the raw backbone coordinate tensor and \n",
    "    protein.sequence holds the protein sequence.\n",
    "    \n",
    "    Using this wrapper function avoids manually defined collation for pytorch DataLoader\n",
    "    \"\"\"\n",
    "    # tokenize_structure converts raw coordinates and sequence into the proper inputs for the structure head.\n",
    "    # It returns a tuple: (coordinates, plddt, structure_tokens)\n",
    "    encoded_coords, plddt, structure_tokens = tokenize_structure(\n",
    "        protein.coordinates,\n",
    "        encoder,\n",
    "        structure_tokenizer,\n",
    "        reference_sequence=protein.sequence,\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    # For our training, we assume the encoded_coords is what the model will consume.\n",
    "    return encoded_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576c0e10-7a37-40cd-979d-9505ef8777e6",
   "metadata": {},
   "source": [
    "•\t`structure_encoder`:\n",
    "The untrained encoder module (from esm/models/vqvae.py) that can be customized for a full train. Shoudl not be direct invocation for finetunes.\n",
    "\n",
    "•\t`encoder`: \n",
    "THeh encoder from pretrained ESM3 model with weights. It is called using ESM's `tokenize_structure` for data ingestion processing\n",
    "\n",
    "•\t`encode_protein` (wrapper function):\n",
    "A higher‑level function that calls ESM's `tokenize_structure` to convert an ESMProtein into the final encoded tensor. \n",
    "This is what we should use in DataLoader. It's an invocation for this training instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e6c8c5b-2c55-44fa-84e5-a277efb26ae8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # ---------------------\n",
    "# # Define a custom Dataset no Encoder\n",
    "# # ---------------------\n",
    "# class ProteinStructureDataset(Dataset):\n",
    "#     def __init__(self, pdb_directory, suffix):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             pdb_directory (str): Directory where the PDB files are stored.\n",
    "#             suffix (str): Suffix to filter files (e.g. \"_train.pdb\" or \"_val.pdb\").\n",
    "#         \"\"\"\n",
    "#         self.pdb_directory = pdb_directory\n",
    "#         self.pdb_files = [os.path.join(pdb_directory, f)\n",
    "#                           for f in os.listdir(pdb_directory) if f.endswith(suffix)]\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.pdb_files)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         pdb_path = self.pdb_files[idx]\n",
    "#         protein = detect_and_process_structure(pdb_path)\n",
    "#         if protein is None:\n",
    "#             raise ValueError(f\"Protein processing failed for {pdb_path}\")\n",
    "#         # Assume that after processing, protein.coordinates is a torch.Tensor\n",
    "#         # representing the ground-truth structure coordinates.\n",
    "#         gt_coords = protein.coordinates  # shape: (L, 3) or (L, 3, ...) as needed\n",
    "#         return protein, gt_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10db24d0-1a94-4429-b235-2aed3758f9a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Revised Dataset that directly returns the encoded protein tensor\n",
    "class ProteinStructureDataset(Dataset):\n",
    "    def __init__(self, pdb_directory: str, suffix: str, encoder):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pdb_directory (str): Directory where the PDB files are stored.\n",
    "            suffix (str): Suffix to filter files (e.g. \"_train.pdb\" or \"_val.pdb\").\n",
    "            encoder (callable): Function that converts an ESMProtein into a tensor.\n",
    "        \"\"\"\n",
    "        self.pdb_directory = pdb_directory\n",
    "        self.pdb_files = [\n",
    "            os.path.join(pdb_directory, f)\n",
    "            for f in os.listdir(pdb_directory) if f.endswith(suffix)\n",
    "        ]\n",
    "        self.encoder = encoder\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pdb_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pdb_path = self.pdb_files[idx]\n",
    "        # Use your existing function to process the PDB into an ESMProtein.\n",
    "        protein = detect_and_process_structure(pdb_path)\n",
    "        if protein is None:\n",
    "            raise ValueError(f\"Protein processing failed for {pdb_path}\")\n",
    "        # Ground truth coordinates remain the same.\n",
    "        gt_coords = protein.coordinates  \n",
    "        # Encode the protein into the tensor expected by the model.\n",
    "        encoded_protein = self.encoder(protein)\n",
    "        return encoded_protein, gt_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46fc1652-e812-41b9-bf52-a1a98a74072d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "# Create datasets and dataloaders\n",
    "# ---------------------\n",
    "train_directory = '/home/jupyter/DATA/hyperbind_train/sabdab/all_structures/train-test-split/'\n",
    "\n",
    "train_dataset = ProteinStructureDataset(train_directory, \"_train.pdb\", encoder=encode_protein)\n",
    "val_dataset   = ProteinStructureDataset(train_directory, \"_val.pdb\", encoder=encode_protein)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c210545a-9ae5-4305-930f-cbe24fda1e57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESM3(\n",
       "  (encoder): EncodeInputs(\n",
       "    (sequence_embed): Embedding(64, 1536)\n",
       "    (plddt_projection): Linear(in_features=16, out_features=1536, bias=True)\n",
       "    (structure_per_res_plddt_projection): Linear(in_features=16, out_features=1536, bias=True)\n",
       "    (structure_tokens_embed): Embedding(4101, 1536)\n",
       "    (ss8_embed): Embedding(11, 1536)\n",
       "    (sasa_embed): Embedding(19, 1536)\n",
       "    (function_embed): ModuleList(\n",
       "      (0-7): 8 x Embedding(260, 192, padding_idx=0)\n",
       "    )\n",
       "    (residue_embed): EmbeddingBag(1478, 1536, mode='sum', padding_idx=0)\n",
       "  )\n",
       "  (transformer): TransformerStack(\n",
       "    (blocks): ModuleList(\n",
       "      (0): UnifiedTransformerBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (layernorm_qkv): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=4608, bias=False)\n",
       "          )\n",
       "          (out_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          (q_ln): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (rotary): RotaryEmbedding()\n",
       "        )\n",
       "        (geom_attn): GeometricReasoningOriginalImpl(\n",
       "          (s_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1536, out_features=3840, bias=False)\n",
       "          (out_proj): Linear(in_features=768, out_features=1536, bias=False)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "          (2): SwiGLU()\n",
       "          (3): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1-47): 47 x UnifiedTransformerBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (layernorm_qkv): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=4608, bias=False)\n",
       "          )\n",
       "          (out_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          (q_ln): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (rotary): RotaryEmbedding()\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "          (2): SwiGLU()\n",
       "          (3): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (output_heads): OutputHeads(\n",
       "    (sequence_head): Sequential(\n",
       "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Linear(in_features=1536, out_features=64, bias=True)\n",
       "    )\n",
       "    (structure_head): Sequential(\n",
       "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Linear(in_features=1536, out_features=4096, bias=True)\n",
       "    )\n",
       "    (ss8_head): Sequential(\n",
       "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Linear(in_features=1536, out_features=11, bias=True)\n",
       "    )\n",
       "    (sasa_head): Sequential(\n",
       "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Linear(in_features=1536, out_features=19, bias=True)\n",
       "    )\n",
       "    (function_head): Sequential(\n",
       "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Linear(in_features=1536, out_features=2080, bias=True)\n",
       "    )\n",
       "    (residue_head): Sequential(\n",
       "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Linear(in_features=1536, out_features=1478, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------\n",
    "# Prepare the model for fine-tuning\n",
    "# ---------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def create_esm3_model():\n",
    "    from esm.pretrained import ESM3_sm_open_v0\n",
    "    model = ESM3_sm_open_v0()  # Now returns only the model object\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model (as in your provided code)\n",
    "model = create_esm3_model()\n",
    "state_dict = torch.load(\n",
    "    \"/home/jupyter/DATA/model_weights/esm3_complete/esm3_sm_open_v1_state_dict.pt\",\n",
    "    map_location=device\n",
    ")\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)\n",
    "\n",
    "# IMPORTANT: Ensure that the model’s structure prediction head is trainable.\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84762a0f-0415-49df-badd-2f6dcfb8df48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# After loading the model and before starting training:\n",
    "for module in model.modules():\n",
    "    from esm.layers.codebook import EMACodebook  # Ensure we refer to the correct class.\n",
    "    if isinstance(module, EMACodebook):\n",
    "         module.freeze_codebook = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48f4e5e1-02d8-495d-b4f8-0a7f4f83cff8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructureTokenDecoder(\n",
       "  (embed): Embedding(4101, 1280)\n",
       "  (decoder_stack): TransformerStack(\n",
       "    (blocks): ModuleList(\n",
       "      (0-29): 30 x UnifiedTransformerBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (layernorm_qkv): Sequential(\n",
       "            (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1280, out_features=3840, bias=False)\n",
       "          )\n",
       "          (out_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (q_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (rotary): RotaryEmbedding()\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=1280, out_features=7168, bias=False)\n",
       "          (2): SwiGLU()\n",
       "          (3): Linear(in_features=3584, out_features=1280, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (affine_output_projection): Dim6RotStructureHead(\n",
       "    (ffn1): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (activation_fn): GELU(approximate='none')\n",
       "    (norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    (proj): Linear(in_features=1280, out_features=23, bias=True)\n",
       "  )\n",
       "  (pairwise_classification_head): PairwisePredictionHead(\n",
       "    (downproject): Linear(in_features=1280, out_features=128, bias=False)\n",
       "    (linear1): Linear(in_features=128, out_features=128, bias=False)\n",
       "    (activation_fn): GELU(approximate='none')\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (linear2): Linear(in_features=128, out_features=224, bias=False)\n",
       "  )\n",
       "  (plddt_head): RegressionHead(\n",
       "    (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (activation_fn): GELU(approximate='none')\n",
       "    (norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    (output): Linear(in_features=1280, out_features=50, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from esm.pretrained import ESM3_structure_decoder_v0\n",
    "\n",
    "# This helper instantiates the structure head (decoder) with its pretrained configuration.\n",
    "structure_head_pretrained = ESM3_structure_decoder_v0(device=\"cuda\")\n",
    "structure_head_pretrained.train()  # set to training mode so that gradients are computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e4fd688-3c38-484b-8a35-ea5f3d5210bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "# Setup training components\n",
    "# ---------------------\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b6f1e24-74eb-46c6-8a05-e3da419a9d0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Detected 2 chains (['G', 'I']). Processing as a multimer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/esm/models/vqvae.py:286: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast(enabled=False):  # type: ignore\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Not implemented",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# --------------\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Validate DataLoader\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# --------------\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Get one batch from the train loader\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m encoded_batch, gt_coords \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncoded batch shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoded_batch\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[7], line 29\u001b[0m, in \u001b[0;36mProteinStructureDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     27\u001b[0m gt_coords \u001b[38;5;241m=\u001b[39m protein\u001b[38;5;241m.\u001b[39mcoordinates  \n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Encode the protein into the tensor expected by the model.\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m encoded_protein \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprotein\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m encoded_protein, gt_coords\n",
      "Cell \u001b[0;32mIn[5], line 15\u001b[0m, in \u001b[0;36mencode_protein\u001b[0;34m(protein)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mEncodes an ESMProtein object into tensors suitable for structure fine-tuning.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mThis uses the tokenize_structure function from the codebase to generate:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mUsing this wrapper function avoids manually defined collation for pytorch DataLoader\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# tokenize_structure converts raw coordinates and sequence into the proper inputs for the structure head.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# It returns a tuple: (coordinates, plddt, structure_tokens)\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m encoded_coords, plddt, structure_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize_structure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprotein\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoordinates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstructure_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreference_sequence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprotein\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# For our training, we assume the encoded_coords is what the model will consume.\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m encoded_coords\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/esm/utils/encoding.py:90\u001b[0m, in \u001b[0;36mtokenize_structure\u001b[0;34m(coordinates, structure_encoder, structure_tokenizer, reference_sequence, add_special_tokens)\u001b[0m\n\u001b[1;32m     88\u001b[0m plddt \u001b[38;5;241m=\u001b[39m plddt\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# (1, L)\u001b[39;00m\n\u001b[1;32m     89\u001b[0m residue_index \u001b[38;5;241m=\u001b[39m residue_index\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# (1, L)\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m _, structure_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mstructure_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoordinates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidue_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresidue_index\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m coordinates \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqueeze(coordinates, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# (L, 37, 3)  # type: ignore\u001b[39;00m\n\u001b[1;32m     94\u001b[0m plddt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqueeze(plddt, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# (L,)  # type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/esm/models/vqvae.py:323\u001b[0m, in \u001b[0;36mStructureTokenEncoder.encode\u001b[0;34m(self, coords, attention_mask, sequence_id, residue_index)\u001b[0m\n\u001b[1;32m    320\u001b[0m z \u001b[38;5;241m=\u001b[39m z\u001b[38;5;241m.\u001b[39mmasked_fill(\u001b[38;5;241m~\u001b[39maffine_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m), \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    321\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_vq_proj(z)\n\u001b[0;32m--> 323\u001b[0m z_q, min_encoding_indices, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodebook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m z_q, min_encoding_indices\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/esm/layers/codebook.py:78\u001b[0m, in \u001b[0;36mEMACodebook.forward\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# EMA codebook update\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfreeze_codebook:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot implemented\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     79\u001b[0m embeddings_st \u001b[38;5;241m=\u001b[39m (embeddings \u001b[38;5;241m-\u001b[39m z)\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m+\u001b[39m z\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings_st, encoding_indices, commitment_loss\n",
      "\u001b[0;31mAssertionError\u001b[0m: Not implemented"
     ]
    }
   ],
   "source": [
    "# --------------\n",
    "# Validate DataLoader\n",
    "# --------------\n",
    "\n",
    "# Get one batch from the train loader\n",
    "batch = next(iter(train_loader))\n",
    "encoded_batch, gt_coords = batch\n",
    "\n",
    "print(\"Encoded batch shape:\", encoded_batch.shape)\n",
    "print(\"Ground truth coordinates shape:\", gt_coords.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84ae3d67-470b-45da-a9f9-a64e1cbfe827",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Detected 2 chains (['D', 'C']). Processing as a multimer.\n",
      "✅ Detected 2 chains (['H', 'L']). Processing as a multimer.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'esm.sdk.api.ESMProtein'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      6\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m proteins, gt_coords \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# proteins is a list of ESMProtein objects; if needed, you may convert them\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# into a batched format that your model accepts.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Move ground truth coordinates to device.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# (If gt_coords is not already a tensor, convert it accordingly.)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     gt_coords \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([p\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(p, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m p \n\u001b[1;32m     13\u001b[0m                              \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m gt_coords])\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Forward pass: obtain predicted structure coordinates.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Depending on your model's implementation, you might need to modify this.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:211\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    212\u001b[0m         collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[1;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:212\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 212\u001b[0m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[1;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:240\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m             \u001b[38;5;66;03m# The sequence type may not support `copy()` / `__setitem__(index, item)`\u001b[39;00m\n\u001b[1;32m    234\u001b[0m             \u001b[38;5;66;03m# or `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[1;32m    235\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    236\u001b[0m                 collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[1;32m    237\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[1;32m    238\u001b[0m             ]\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'esm.sdk.api.ESMProtein'>"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "# Training loop\n",
    "# ---------------------\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for proteins, gt_coords in train_loader:\n",
    "        # proteins is a list of ESMProtein objects; if needed, you may convert them\n",
    "        # into a batched format that your model accepts.\n",
    "        # Move ground truth coordinates to device.\n",
    "        # (If gt_coords is not already a tensor, convert it accordingly.)\n",
    "        gt_coords = torch.stack([p.to(device) if isinstance(p, torch.Tensor) else p \n",
    "                                 for p in gt_coords]).float()\n",
    "        \n",
    "        # Forward pass: obtain predicted structure coordinates.\n",
    "        # Depending on your model's implementation, you might need to modify this.\n",
    "        pred_coords = forward_structure(model, proteins)\n",
    "        # Ensure pred_coords is on the same device and has the same shape as gt_coords.\n",
    "        pred_coords = pred_coords.to(device)\n",
    "        \n",
    "        loss = loss_fn(pred_coords, gt_coords)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for proteins, gt_coords in val_loader:\n",
    "            gt_coords = torch.stack([p.to(device) if isinstance(p, torch.Tensor) else p \n",
    "                                     for p in gt_coords]).float()\n",
    "            pred_coords = forward_structure(model, proteins)\n",
    "            pred_coords = pred_coords.to(device)\n",
    "            loss = loss_fn(pred_coords, gt_coords)\n",
    "            val_loss += loss.item()\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "print(\"Fine-tuning complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522db800-bde1-4629-a959-037f68ff4f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13 (Local)",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
