{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43788772-9351-4061-b7b2-740b87d024c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e26a1d0c-cf01-456a-a79b-822a930e9432",
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts_path = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\", \"scripts\"))\n",
    "\n",
    "if scripts_path not in sys.path:\n",
    "    sys.path.insert(0, scripts_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26241f28-ca9d-4ad3-965d-3990442eb871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b28fb31-a852-4ca7-b31e-4d8b01a998ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath = \"/Volumes/LaCie_d2_Professional_Media/absolut_antibody/PerClass/RawBindingsPerClassMurine/extracted/processed/\"\n",
    "df = data_ingestion.load_processed_csvs(dirpath, ['3KR3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17b0bea6-e763-4d4a-91a2-167be6ea5ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = os.path.join(dirpath,'3KR3_D_consolidated.csv')\n",
    "df = pd.read_csv(fpath,index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b1298e9-81a6-4e02-a902-4d5a1ca7cbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import esm\n",
    "\n",
    "# Load the ESM-2 variant (adjust the variant name if desired)\n",
    "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval()  # set to evaluation mode\n",
    "\n",
    "# Prepare sample sequences as a list of (name, sequence) tuples.\n",
    "data = [\n",
    "    (\"protein1\", \"MKTAYIAKQRQISFVKSHFSRQLEERLGLIEVQ\"),\n",
    "    (\"protein2\", \"MGSSHHHHHHSSGLVPRGSHMASMTGGQQMGRDLYDDDDKDP\"),\n",
    "    # Add more sequences as needed\n",
    "]\n",
    "\n",
    "# Convert the data into batch format required by the model.\n",
    "batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(\"cuda\")\n",
    "    batch_tokens = batch_tokens.to(\"cuda\")\n",
    "\n",
    "# For ESM-2, use the last layer (model.num_layers) for representations.\n",
    "layer = model.num_layers\n",
    "with torch.no_grad():\n",
    "    results = model(batch_tokens, repr_layers=[layer], return_contacts=False)\n",
    "token_representations = results[\"representations\"][layer]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099f7c7d-ed99-4d08-b48b-3bcf94257878",
   "metadata": {},
   "source": [
    "## Assume that the ESM-2 model is already loaded and used to generate token representations.\n",
    "For example:\n",
    "results = model(batch_tokens, repr_layers=[model.num_layers], return_contacts=False)\n",
    "token_representations = results[\"representations\"][model.num_layers]\n",
    "and that \"data\" is a list of (name, sequence) tuples.\n",
    "\n",
    "token_representations is a tensor of shape [batch_size, L, D] where L is the number of tokens\n",
    "(including special tokens) and D is the hidden dimension (assumed to be 1280 here).\n",
    "\n",
    "•\tSimple Average Embedding:\n",
    "We average the token embeddings corresponding to the actual amino acid sequence (excluding the first special token) because the start token might not capture the detailed information in the sequence. We then project this vector from 1280 dimensions to 1024 (if desired) using a linear layer.\n",
    "\t\n",
    " •\tAdvanced Embeddings:\n",
    " \t1.\tCLS Token Only:\n",
    "Use only the model’s [CLS] (or start-of-sequence) token as the sequence representation. This is the standard approach in many transformer models for classification tasks, though it sometimes loses finer-grained information.\n",
    "\n",
    "\t2.\tMean Pooling:\n",
    "Compute the average of all token representations (typically excluding the special tokens). This approach tends to capture the overall content of the sequence and is often more robust than the CLS token alone.\n",
    "\n",
    "\t3.\tCLS, MIN, MAX, MEAN Concatenation:\n",
    "Concatenate the CLS token with statistics computed over the sequence tokens:\n",
    "\t•\tMIN: Elementwise minimum over token embeddings.\n",
    "\t•\tMAX: Elementwise maximum.\n",
    "\t•\tMEAN: Elementwise average.\n",
    "This yields a vector with 4×(hidden dimension) dimensions (e.g., 4×1280 = 5120 dims for an ESM model with 1280 hidden units). This strategy is popular because it provides multiple views of the sequence’s distribution.\n",
    "\n",
    "\t4.\tCLS, MEAN, and Standard Deviation Concatenation:\n",
    "Instead of using min and max, you can compute the elementwise standard deviation of the token embeddings along with the CLS token and the mean. This produces a 3×(hidden dimension) vector (e.g., 3×1280 = 3840 dims) and captures both central tendency and dispersion.\n",
    "\n",
    "\t5.\tLayer-wise Concatenation:\n",
    "Instead of taking only the last layer’s output, you can concatenate representations from multiple layers (e.g., last and penultimate layers) to capture multi-scale information. For example, concatenating two layers would double the embedding size.\n",
    "\n",
    "\t6.\tAttention-based Pooling:\n",
    "Learn attention weights for each token and compute a weighted average of token embeddings. You can combine this with the CLS token or other summary statistics.\n",
    "\n",
    "    We concatenate:\n",
    "\n",
    "    •\tThe CLS token (position 0),\n",
    "\n",
    "    •\tThe elementwise minimum across the sequence tokens,\n",
    "\n",
    "    •\tThe elementwise maximum across the sequence tokens,\n",
    "\n",
    "    •\tThe elementwise mean across the sequence tokens.\n",
    "\n",
    "This yields a vector of size 4 × 1280 = 5120 dimensions. This approach leverages multiple statistical summaries of the token embeddings, potentially capturing richer information about the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "739308c9-f482-4ae5-9d7a-f06cea3e803c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple embedding (projected to 1024 dims) shape: (1024,)\n"
     ]
    }
   ],
   "source": [
    "# --- Option 1: Simple Average Embedding (excluding CLS token) ---\n",
    "simple_embeddings = []\n",
    "for i, (name, seq) in enumerate(data):\n",
    "    # Exclude the first token (CLS or start-of-sequence)\n",
    "    avg_embedding = token_representations[i, 1:len(seq)+1].mean(0)\n",
    "    simple_embeddings.append(avg_embedding)\n",
    "\n",
    "# Optionally project to a fixed dimension (e.g., 1024) if desired.\n",
    "hidden_dim = simple_embeddings[0].shape[0]  # e.g., 1280\n",
    "target_dim = 1024\n",
    "if hidden_dim != target_dim:\n",
    "    projector = nn.Linear(hidden_dim, target_dim)\n",
    "    if torch.cuda.is_available():\n",
    "        projector = projector.to(\"cuda\")\n",
    "    simple_embeddings_1024 = [\n",
    "        projector(e.unsqueeze(0)).squeeze(0).detach().cpu().numpy() for e in simple_embeddings\n",
    "    ]\n",
    "else:\n",
    "    simple_embeddings_1024 = [e.cpu().numpy() for e in simple_embeddings]\n",
    "\n",
    "print(\"Simple embedding (projected to 1024 dims) shape:\", simple_embeddings_1024[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec1c1e6e-fe4d-49ea-88bf-6c1df3d61386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Option 2: Advanced Embedding: CLS, MIN, MAX, MEAN Concatenation ---\n",
    "advanced_embeddings = []\n",
    "for i, (name, seq) in enumerate(data):\n",
    "    # CLS token is at index 0\n",
    "    cls_token = token_representations[i, 0]\n",
    "    # Sequence tokens: indices 1 to len(seq)+1 (excluding special tokens)\n",
    "    seq_tokens = token_representations[i, 1:len(seq)+1]\n",
    "    # Compute elementwise min, max, and mean over the sequence tokens.\n",
    "    min_vec = seq_tokens.min(dim=0)[0]\n",
    "    max_vec = seq_tokens.max(dim=0)[0]\n",
    "    mean_vec = seq_tokens.mean(dim=0)\n",
    "    # Concatenate the CLS token, min, max, and mean vectors.\n",
    "    concat_vec = torch.cat([cls_token, min_vec, max_vec, mean_vec], dim=0)\n",
    "    advanced_embeddings.append(concat_vec.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "874607f9-8b73-4e9d-a45b-c9fd717d77c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5120,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advanced_embeddings[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31d88141-f831-4201-a8be-09088c886cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced (CLS, MIN, MAX, MEAN) embedding shape: (5120,)\n",
      "Advanced (CLS, MEAN, STD) embedding shape: (3840,)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Option 1: CLS, MIN, MAX, MEAN Concatenation (4 x D dimensions)\n",
    "# ---------------------------\n",
    "advanced_embeddings_concat = []\n",
    "for i, (name, seq) in enumerate(data):\n",
    "    cls_token = token_representations[i, 0]  # CLS token at index 0\n",
    "    # Use only the tokens corresponding to the sequence (exclude special tokens)\n",
    "    seq_tokens = token_representations[i, 1:len(seq)+1]\n",
    "    min_vec = seq_tokens.min(dim=0)[0]\n",
    "    max_vec = seq_tokens.max(dim=0)[0]\n",
    "    mean_vec = seq_tokens.mean(dim=0)\n",
    "    concat_vec = torch.cat([cls_token, min_vec, max_vec, mean_vec], dim=0)\n",
    "    advanced_embeddings_concat.append(concat_vec.cpu().numpy())\n",
    "    \n",
    "print(\"Advanced (CLS, MIN, MAX, MEAN) embedding shape:\", advanced_embeddings_concat[0].shape)\n",
    "# Expected shape: 4 * D, e.g., 4 * 1280 = 5120 dims.\n",
    "\n",
    "# ---------------------------\n",
    "# Option 2: CLS, MEAN, STD Concatenation (3 x D dimensions)\n",
    "# ---------------------------\n",
    "advanced_embeddings_std = []\n",
    "for i, (name, seq) in enumerate(data):\n",
    "    cls_token = token_representations[i, 0]  # CLS token at index 0\n",
    "    seq_tokens = token_representations[i, 1:len(seq)+1]\n",
    "    mean_vec = seq_tokens.mean(dim=0)\n",
    "    std_vec = seq_tokens.std(dim=0)\n",
    "    concat_vec = torch.cat([cls_token, mean_vec, std_vec], dim=0)\n",
    "    advanced_embeddings_std.append(concat_vec.cpu().numpy())\n",
    "    \n",
    "print(\"Advanced (CLS, MEAN, STD) embedding shape:\", advanced_embeddings_std[0].shape)\n",
    "# Expected shape: 3 * D, e.g., 3 * 1280 = 3840 dims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec51e9ed-d488-46f1-8433-b67c658caee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESM-2 sequence embedding shape: (1280,)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tree'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 33\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mESM-2 sequence embedding shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, seq_embedding\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# 2. Generate Structure Prediction (and Structural Features) with ESMFold\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Load the ESMFold model (ESMFold v1)\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m esmfold_model, esmfold_alphabet \u001b[38;5;241m=\u001b[39m \u001b[43mesm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mesmfold_v1\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m esmfold_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# ESMFold takes a raw sequence as input.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/esm/pretrained.py:419\u001b[0m, in \u001b[0;36mesmfold_v1\u001b[0;34m()\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mesmfold_v1\u001b[39m():\n\u001b[1;32m    412\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;124;03m    ESMFold v1 model using 3B ESM-2, 48 folding blocks.\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;124;03m    ESMFold provides fast high accuracy atomic level structure prediction\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03m    protein sequence.\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mesm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mesmfold\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpretrained\u001b[39;00m\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m esm\u001b[38;5;241m.\u001b[39mesmfold\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mpretrained\u001b[38;5;241m.\u001b[39mesmfold_v1()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/esm/esmfold/v1/pretrained.py:10\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mesm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mesmfold\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mesmfold\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ESMFold\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_model\u001b[39m(model_name):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# local, treat as filepath\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/esm/esmfold/v1/esmfold.py:17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mesm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Alphabet\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mesm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mesmfold\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcategorical_mixture\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m categorical_lddt\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mesm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mesmfold\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     18\u001b[0m     batch_encode_sequences,\n\u001b[1;32m     19\u001b[0m     collate_dense_tensors,\n\u001b[1;32m     20\u001b[0m     output_to_pdb,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mesm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mesmfold\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrunk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FoldingTrunk, FoldingTrunkConfig\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenfold\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_transforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_atom14_masks\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/esm/esmfold/v1/misc.py:12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meinops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rearrange, repeat\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenfold\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m residue_constants\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenfold\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotein\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Protein \u001b[38;5;28;01mas\u001b[39;00m OFProtein\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenfold\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotein\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_pdb\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openfold/np/residue_constants.py:25\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resources\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtree\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Distance from one CA to next CA [trans configuration: omega = 180].\u001b[39;00m\n\u001b[1;32m     29\u001b[0m ca_ca \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3.80209737096\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tree'"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------\n",
    "# 1. Generate Sequence Embeddings with ESM-2\n",
    "# -----------------------\n",
    "\n",
    "# Load the ESM-2 model (variant esm2_t33_650M_UR50D) and its alphabet.\n",
    "esm2_model, esm2_alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "esm2_batch_converter = esm2_alphabet.get_batch_converter()\n",
    "esm2_model.eval()\n",
    "\n",
    "# Sample protein sequence\n",
    "sequence = \"MKTAYIAKQRQISFVKSHFSRQLEERLGLIEVQ\"\n",
    "data = [(\"protein1\", sequence)]\n",
    "\n",
    "# Convert data to batch format.\n",
    "batch_labels, batch_strs, batch_tokens = esm2_batch_converter(data)\n",
    "if torch.cuda.is_available():\n",
    "    esm2_model = esm2_model.to(\"cuda\")\n",
    "    batch_tokens = batch_tokens.to(\"cuda\")\n",
    "\n",
    "# Generate sequence embeddings (using the last layer representation).\n",
    "with torch.no_grad():\n",
    "    esm2_results = esm2_model(batch_tokens, repr_layers=[esm2_model.num_layers], return_contacts=False)\n",
    "# Average token representations (exclude special tokens) to get a single embedding vector.\n",
    "seq_embedding = esm2_results[\"representations\"][esm2_model.num_layers][0, 1:len(sequence)+1].mean(0).cpu().numpy()\n",
    "print(\"ESM-2 sequence embedding shape:\", seq_embedding.shape)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 2. Generate Structure Prediction (and Structural Features) with ESMFold\n",
    "# -----------------------\n",
    "\n",
    "# Load the ESMFold model (ESMFold v1)\n",
    "esmfold_model, esmfold_alphabet = esm.pretrained.esmfold_v1()\n",
    "esmfold_model.eval()\n",
    "\n",
    "# ESMFold takes a raw sequence as input.\n",
    "with torch.no_grad():\n",
    "    # ESMFold returns a dictionary that includes predicted 3D coordinates (under key 'coords')\n",
    "    # and other outputs such as pLDDT confidence scores.\n",
    "    fold_result = esmfold_model(sequence)\n",
    "    \n",
    "# Extract the predicted structure coordinates.\n",
    "coords = fold_result['coords']  # shape typically [L, 3] (for a sequence of length L)\n",
    "print(\"Predicted structure coordinates shape:\", coords.shape)\n",
    "\n",
    "# Optionally, if ESMFold exposes internal representations (e.g., hidden states),\n",
    "# you could extract them and aggregate them as \"structural embeddings\".\n",
    "# For example, if fold_result had a key \"hidden_states\", you might do:\n",
    "# struct_embedding = fold_result[\"hidden_states\"][-1].mean(dim=0).cpu().numpy()\n",
    "# (This depends on the ESMFold API and whether such representations are available.)\n",
    "  \n",
    "# -----------------------\n",
    "# 3. (Optional) Fuse Sequence and Structure Embeddings\n",
    "# -----------------------\n",
    "\n",
    "# One common strategy is to concatenate the sequence embedding and a structural embedding.\n",
    "# For demonstration, assume we only use the sequence embedding and the flattened structure coordinates.\n",
    "# You might also compute summary statistics (e.g., mean, variance) over the coordinates.\n",
    "struct_embedding = coords.mean(axis=0)  # Simple example: mean coordinate per dimension.\n",
    "print(\"Mean structural embedding shape:\", struct_embedding.shape)\n",
    "\n",
    "# Concatenate both embeddings.\n",
    "import numpy as np\n",
    "combined_embedding = np.concatenate([seq_embedding, struct_embedding])\n",
    "print(\"Combined embedding shape:\", combined_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2efe35b3-fb1e-4fb1-bc54-1ff7be8d6323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/aqlaboratory/openfold.git\n",
      "  Cloning https://github.com/aqlaboratory/openfold.git to /private/var/folders/lg/dh15rq991nb0srz_jbgtk8f80000gn/T/pip-req-build-vdii9_3i\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/aqlaboratory/openfold.git /private/var/folders/lg/dh15rq991nb0srz_jbgtk8f80000gn/T/pip-req-build-vdii9_3i\n",
      "  Resolved https://github.com/aqlaboratory/openfold.git to commit a1192c8d3a0f3004b1284aaf6437681e6b558c10\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: openfold\n",
      "  Building wheel for openfold (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for openfold: filename=openfold-2.0.0-cp310-cp310-macosx_14_0_arm64.whl size=316729 sha256=5f970f398e5006f5316f3c8dd9a14840a7cf1b8adc11d36cdcd53495a68626be\n",
      "  Stored in directory: /private/var/folders/lg/dh15rq991nb0srz_jbgtk8f80000gn/T/pip-ephem-wheel-cache-4oav79mj/wheels/c8/68/f7/33ea4cb0f20d1303e1d5cbbdd73fdd0d5147843868ca49a9af\n",
      "Successfully built openfold\n",
      "Installing collected packages: openfold\n",
      "Successfully installed openfold-2.0.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/aqlaboratory/openfold.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0242c262-8038-4b8b-b1ee-387ddbf722d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
