{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfecf27e-d651-4bb6-95dd-f3fcfe0b7c73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Set a custom cache directory for torch.hub.\n",
    "# This tells torch.hub where to store (and look for) downloaded model files.\n",
    "import torch\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8c8304c-60a5-496a-b79e-195e1e0baa65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making a call to huggingface to pull model weights and storing in Torch Hub cache directory: /home/jupyter/.cache/torch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cc96431385c4438bab19e01eb23aba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 22 files:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. Import and load ESM3 from the native codebase.\n",
    "# Here, we use the native ESM3 API from the EvolutionaryScale/esm repository.\n",
    "# Make sure you have cloned/installed the ESM repository (see README in the repo).\n",
    "from esm.models.esm3 import ESM3\n",
    "from esm.tokenization.sequence_tokenizer import EsmSequenceTokenizer\n",
    "\n",
    "print(f\"making a call to huggingface to pull model weights and storing in Torch Hub cache directory: {torch.hub._get_torch_home()}\")\n",
    "# The model identifier should match one available in the repository.\n",
    "model_id = \"esm3_sm_open_v1\"\n",
    "model = ESM3.from_pretrained(model_id,device=torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60bad5d-6a9f-4d44-b3f6-2e961f5da484",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Move the model to GPU if available; otherwise, use CPU.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "# check the number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total number of parameters:\", total_params)\n",
    "\n",
    "# check state dict\n",
    "state_dict = model.state_dict()\n",
    "for key in state_dict:\n",
    "    print(f\"{key}: {state_dict[key].shape}\")\n",
    "\n",
    "# set to eval mode\n",
    "model.eval()  # Set model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a0f6b7d-42d1-48dc-9d7e-9c04b15938ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "backbone_model = copy.deepcopy(model)\n",
    "# Remove the output heads.\n",
    "# For example, if your model stores its output heads in an attribute named 'output_heads',\n",
    "# you can remove them by setting that attribute to None (or an empty dict).\n",
    "if hasattr(backbone_model, \"output_heads\"):\n",
    "    backbone_model.output_heads = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "efa7e8f9-82c3-4124-a648-0a723d11daea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESM3(\n",
       "  (encoder): EncodeInputs(\n",
       "    (sequence_embed): Embedding(64, 1536)\n",
       "    (plddt_projection): Linear(in_features=16, out_features=1536, bias=True)\n",
       "    (structure_per_res_plddt_projection): Linear(in_features=16, out_features=1536, bias=True)\n",
       "    (structure_tokens_embed): Embedding(4101, 1536)\n",
       "    (ss8_embed): Embedding(11, 1536)\n",
       "    (sasa_embed): Embedding(19, 1536)\n",
       "    (function_embed): ModuleList(\n",
       "      (0-7): 8 x Embedding(260, 192, padding_idx=0)\n",
       "    )\n",
       "    (residue_embed): EmbeddingBag(1478, 1536, mode='sum', padding_idx=0)\n",
       "  )\n",
       "  (transformer): TransformerStack(\n",
       "    (blocks): ModuleList(\n",
       "      (0): UnifiedTransformerBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (layernorm_qkv): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=4608, bias=False)\n",
       "          )\n",
       "          (out_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          (q_ln): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (rotary): RotaryEmbedding()\n",
       "        )\n",
       "        (geom_attn): GeometricReasoningOriginalImpl(\n",
       "          (s_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1536, out_features=3840, bias=False)\n",
       "          (out_proj): Linear(in_features=768, out_features=1536, bias=False)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "          (2): SwiGLU()\n",
       "          (3): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1-47): 47 x UnifiedTransformerBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (layernorm_qkv): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=4608, bias=False)\n",
       "          )\n",
       "          (out_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          (q_ln): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (rotary): RotaryEmbedding()\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=1536, out_features=8192, bias=False)\n",
       "          (2): SwiGLU()\n",
       "          (3): Linear(in_features=4096, out_features=1536, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (output_heads): None\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backbone_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "98360656-7283-44a4-8147-31e333ac7987",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone keys:\n",
      "encoder.function_embed.0.weight\n",
      "encoder.function_embed.1.weight\n",
      "encoder.function_embed.2.weight\n",
      "encoder.function_embed.3.weight\n",
      "encoder.function_embed.4.weight\n",
      "encoder.function_embed.5.weight\n",
      "encoder.function_embed.6.weight\n",
      "encoder.function_embed.7.weight\n",
      "encoder.plddt_projection.bias\n",
      "encoder.plddt_projection.weight\n",
      "encoder.residue_embed.weight\n",
      "encoder.sasa_embed.weight\n",
      "encoder.sequence_embed.weight\n",
      "encoder.ss8_embed.weight\n",
      "encoder.structure_per_res_plddt_projection.bias\n",
      "encoder.structure_per_res_plddt_projection.weight\n",
      "encoder.structure_tokens_embed.weight\n",
      "transformer.blocks.0.attn.k_ln.weight\n",
      "transformer.blocks.0.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.0.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.0.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.0.attn.out_proj.weight\n",
      "transformer.blocks.0.attn.q_ln.weight\n",
      "transformer.blocks.0.ffn.0.bias\n",
      "transformer.blocks.0.ffn.0.weight\n",
      "transformer.blocks.0.ffn.1.weight\n",
      "transformer.blocks.0.ffn.3.weight\n",
      "transformer.blocks.0.geom_attn.distance_scale_per_head\n",
      "transformer.blocks.0.geom_attn.out_proj.weight\n",
      "transformer.blocks.0.geom_attn.proj.weight\n",
      "transformer.blocks.0.geom_attn.rotation_scale_per_head\n",
      "transformer.blocks.0.geom_attn.s_norm.weight\n",
      "transformer.blocks.1.attn.k_ln.weight\n",
      "transformer.blocks.1.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.1.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.1.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.1.attn.out_proj.weight\n",
      "transformer.blocks.1.attn.q_ln.weight\n",
      "transformer.blocks.1.ffn.0.bias\n",
      "transformer.blocks.1.ffn.0.weight\n",
      "transformer.blocks.1.ffn.1.weight\n",
      "transformer.blocks.1.ffn.3.weight\n",
      "transformer.blocks.10.attn.k_ln.weight\n",
      "transformer.blocks.10.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.10.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.10.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.10.attn.out_proj.weight\n",
      "transformer.blocks.10.attn.q_ln.weight\n",
      "transformer.blocks.10.ffn.0.bias\n",
      "transformer.blocks.10.ffn.0.weight\n",
      "transformer.blocks.10.ffn.1.weight\n",
      "transformer.blocks.10.ffn.3.weight\n",
      "transformer.blocks.11.attn.k_ln.weight\n",
      "transformer.blocks.11.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.11.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.11.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.11.attn.out_proj.weight\n",
      "transformer.blocks.11.attn.q_ln.weight\n",
      "transformer.blocks.11.ffn.0.bias\n",
      "transformer.blocks.11.ffn.0.weight\n",
      "transformer.blocks.11.ffn.1.weight\n",
      "transformer.blocks.11.ffn.3.weight\n",
      "transformer.blocks.12.attn.k_ln.weight\n",
      "transformer.blocks.12.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.12.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.12.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.12.attn.out_proj.weight\n",
      "transformer.blocks.12.attn.q_ln.weight\n",
      "transformer.blocks.12.ffn.0.bias\n",
      "transformer.blocks.12.ffn.0.weight\n",
      "transformer.blocks.12.ffn.1.weight\n",
      "transformer.blocks.12.ffn.3.weight\n",
      "transformer.blocks.13.attn.k_ln.weight\n",
      "transformer.blocks.13.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.13.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.13.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.13.attn.out_proj.weight\n",
      "transformer.blocks.13.attn.q_ln.weight\n",
      "transformer.blocks.13.ffn.0.bias\n",
      "transformer.blocks.13.ffn.0.weight\n",
      "transformer.blocks.13.ffn.1.weight\n",
      "transformer.blocks.13.ffn.3.weight\n",
      "transformer.blocks.14.attn.k_ln.weight\n",
      "transformer.blocks.14.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.14.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.14.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.14.attn.out_proj.weight\n",
      "transformer.blocks.14.attn.q_ln.weight\n",
      "transformer.blocks.14.ffn.0.bias\n",
      "transformer.blocks.14.ffn.0.weight\n",
      "transformer.blocks.14.ffn.1.weight\n",
      "transformer.blocks.14.ffn.3.weight\n",
      "transformer.blocks.15.attn.k_ln.weight\n",
      "transformer.blocks.15.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.15.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.15.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.15.attn.out_proj.weight\n",
      "transformer.blocks.15.attn.q_ln.weight\n",
      "transformer.blocks.15.ffn.0.bias\n",
      "transformer.blocks.15.ffn.0.weight\n",
      "transformer.blocks.15.ffn.1.weight\n",
      "transformer.blocks.15.ffn.3.weight\n",
      "transformer.blocks.16.attn.k_ln.weight\n",
      "transformer.blocks.16.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.16.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.16.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.16.attn.out_proj.weight\n",
      "transformer.blocks.16.attn.q_ln.weight\n",
      "transformer.blocks.16.ffn.0.bias\n",
      "transformer.blocks.16.ffn.0.weight\n",
      "transformer.blocks.16.ffn.1.weight\n",
      "transformer.blocks.16.ffn.3.weight\n",
      "transformer.blocks.17.attn.k_ln.weight\n",
      "transformer.blocks.17.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.17.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.17.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.17.attn.out_proj.weight\n",
      "transformer.blocks.17.attn.q_ln.weight\n",
      "transformer.blocks.17.ffn.0.bias\n",
      "transformer.blocks.17.ffn.0.weight\n",
      "transformer.blocks.17.ffn.1.weight\n",
      "transformer.blocks.17.ffn.3.weight\n",
      "transformer.blocks.18.attn.k_ln.weight\n",
      "transformer.blocks.18.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.18.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.18.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.18.attn.out_proj.weight\n",
      "transformer.blocks.18.attn.q_ln.weight\n",
      "transformer.blocks.18.ffn.0.bias\n",
      "transformer.blocks.18.ffn.0.weight\n",
      "transformer.blocks.18.ffn.1.weight\n",
      "transformer.blocks.18.ffn.3.weight\n",
      "transformer.blocks.19.attn.k_ln.weight\n",
      "transformer.blocks.19.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.19.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.19.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.19.attn.out_proj.weight\n",
      "transformer.blocks.19.attn.q_ln.weight\n",
      "transformer.blocks.19.ffn.0.bias\n",
      "transformer.blocks.19.ffn.0.weight\n",
      "transformer.blocks.19.ffn.1.weight\n",
      "transformer.blocks.19.ffn.3.weight\n",
      "transformer.blocks.2.attn.k_ln.weight\n",
      "transformer.blocks.2.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.2.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.2.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.2.attn.out_proj.weight\n",
      "transformer.blocks.2.attn.q_ln.weight\n",
      "transformer.blocks.2.ffn.0.bias\n",
      "transformer.blocks.2.ffn.0.weight\n",
      "transformer.blocks.2.ffn.1.weight\n",
      "transformer.blocks.2.ffn.3.weight\n",
      "transformer.blocks.20.attn.k_ln.weight\n",
      "transformer.blocks.20.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.20.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.20.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.20.attn.out_proj.weight\n",
      "transformer.blocks.20.attn.q_ln.weight\n",
      "transformer.blocks.20.ffn.0.bias\n",
      "transformer.blocks.20.ffn.0.weight\n",
      "transformer.blocks.20.ffn.1.weight\n",
      "transformer.blocks.20.ffn.3.weight\n",
      "transformer.blocks.21.attn.k_ln.weight\n",
      "transformer.blocks.21.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.21.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.21.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.21.attn.out_proj.weight\n",
      "transformer.blocks.21.attn.q_ln.weight\n",
      "transformer.blocks.21.ffn.0.bias\n",
      "transformer.blocks.21.ffn.0.weight\n",
      "transformer.blocks.21.ffn.1.weight\n",
      "transformer.blocks.21.ffn.3.weight\n",
      "transformer.blocks.22.attn.k_ln.weight\n",
      "transformer.blocks.22.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.22.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.22.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.22.attn.out_proj.weight\n",
      "transformer.blocks.22.attn.q_ln.weight\n",
      "transformer.blocks.22.ffn.0.bias\n",
      "transformer.blocks.22.ffn.0.weight\n",
      "transformer.blocks.22.ffn.1.weight\n",
      "transformer.blocks.22.ffn.3.weight\n",
      "transformer.blocks.23.attn.k_ln.weight\n",
      "transformer.blocks.23.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.23.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.23.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.23.attn.out_proj.weight\n",
      "transformer.blocks.23.attn.q_ln.weight\n",
      "transformer.blocks.23.ffn.0.bias\n",
      "transformer.blocks.23.ffn.0.weight\n",
      "transformer.blocks.23.ffn.1.weight\n",
      "transformer.blocks.23.ffn.3.weight\n",
      "transformer.blocks.24.attn.k_ln.weight\n",
      "transformer.blocks.24.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.24.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.24.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.24.attn.out_proj.weight\n",
      "transformer.blocks.24.attn.q_ln.weight\n",
      "transformer.blocks.24.ffn.0.bias\n",
      "transformer.blocks.24.ffn.0.weight\n",
      "transformer.blocks.24.ffn.1.weight\n",
      "transformer.blocks.24.ffn.3.weight\n",
      "transformer.blocks.25.attn.k_ln.weight\n",
      "transformer.blocks.25.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.25.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.25.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.25.attn.out_proj.weight\n",
      "transformer.blocks.25.attn.q_ln.weight\n",
      "transformer.blocks.25.ffn.0.bias\n",
      "transformer.blocks.25.ffn.0.weight\n",
      "transformer.blocks.25.ffn.1.weight\n",
      "transformer.blocks.25.ffn.3.weight\n",
      "transformer.blocks.26.attn.k_ln.weight\n",
      "transformer.blocks.26.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.26.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.26.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.26.attn.out_proj.weight\n",
      "transformer.blocks.26.attn.q_ln.weight\n",
      "transformer.blocks.26.ffn.0.bias\n",
      "transformer.blocks.26.ffn.0.weight\n",
      "transformer.blocks.26.ffn.1.weight\n",
      "transformer.blocks.26.ffn.3.weight\n",
      "transformer.blocks.27.attn.k_ln.weight\n",
      "transformer.blocks.27.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.27.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.27.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.27.attn.out_proj.weight\n",
      "transformer.blocks.27.attn.q_ln.weight\n",
      "transformer.blocks.27.ffn.0.bias\n",
      "transformer.blocks.27.ffn.0.weight\n",
      "transformer.blocks.27.ffn.1.weight\n",
      "transformer.blocks.27.ffn.3.weight\n",
      "transformer.blocks.28.attn.k_ln.weight\n",
      "transformer.blocks.28.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.28.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.28.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.28.attn.out_proj.weight\n",
      "transformer.blocks.28.attn.q_ln.weight\n",
      "transformer.blocks.28.ffn.0.bias\n",
      "transformer.blocks.28.ffn.0.weight\n",
      "transformer.blocks.28.ffn.1.weight\n",
      "transformer.blocks.28.ffn.3.weight\n",
      "transformer.blocks.29.attn.k_ln.weight\n",
      "transformer.blocks.29.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.29.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.29.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.29.attn.out_proj.weight\n",
      "transformer.blocks.29.attn.q_ln.weight\n",
      "transformer.blocks.29.ffn.0.bias\n",
      "transformer.blocks.29.ffn.0.weight\n",
      "transformer.blocks.29.ffn.1.weight\n",
      "transformer.blocks.29.ffn.3.weight\n",
      "transformer.blocks.3.attn.k_ln.weight\n",
      "transformer.blocks.3.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.3.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.3.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.3.attn.out_proj.weight\n",
      "transformer.blocks.3.attn.q_ln.weight\n",
      "transformer.blocks.3.ffn.0.bias\n",
      "transformer.blocks.3.ffn.0.weight\n",
      "transformer.blocks.3.ffn.1.weight\n",
      "transformer.blocks.3.ffn.3.weight\n",
      "transformer.blocks.30.attn.k_ln.weight\n",
      "transformer.blocks.30.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.30.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.30.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.30.attn.out_proj.weight\n",
      "transformer.blocks.30.attn.q_ln.weight\n",
      "transformer.blocks.30.ffn.0.bias\n",
      "transformer.blocks.30.ffn.0.weight\n",
      "transformer.blocks.30.ffn.1.weight\n",
      "transformer.blocks.30.ffn.3.weight\n",
      "transformer.blocks.31.attn.k_ln.weight\n",
      "transformer.blocks.31.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.31.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.31.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.31.attn.out_proj.weight\n",
      "transformer.blocks.31.attn.q_ln.weight\n",
      "transformer.blocks.31.ffn.0.bias\n",
      "transformer.blocks.31.ffn.0.weight\n",
      "transformer.blocks.31.ffn.1.weight\n",
      "transformer.blocks.31.ffn.3.weight\n",
      "transformer.blocks.32.attn.k_ln.weight\n",
      "transformer.blocks.32.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.32.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.32.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.32.attn.out_proj.weight\n",
      "transformer.blocks.32.attn.q_ln.weight\n",
      "transformer.blocks.32.ffn.0.bias\n",
      "transformer.blocks.32.ffn.0.weight\n",
      "transformer.blocks.32.ffn.1.weight\n",
      "transformer.blocks.32.ffn.3.weight\n",
      "transformer.blocks.33.attn.k_ln.weight\n",
      "transformer.blocks.33.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.33.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.33.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.33.attn.out_proj.weight\n",
      "transformer.blocks.33.attn.q_ln.weight\n",
      "transformer.blocks.33.ffn.0.bias\n",
      "transformer.blocks.33.ffn.0.weight\n",
      "transformer.blocks.33.ffn.1.weight\n",
      "transformer.blocks.33.ffn.3.weight\n",
      "transformer.blocks.34.attn.k_ln.weight\n",
      "transformer.blocks.34.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.34.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.34.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.34.attn.out_proj.weight\n",
      "transformer.blocks.34.attn.q_ln.weight\n",
      "transformer.blocks.34.ffn.0.bias\n",
      "transformer.blocks.34.ffn.0.weight\n",
      "transformer.blocks.34.ffn.1.weight\n",
      "transformer.blocks.34.ffn.3.weight\n",
      "transformer.blocks.35.attn.k_ln.weight\n",
      "transformer.blocks.35.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.35.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.35.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.35.attn.out_proj.weight\n",
      "transformer.blocks.35.attn.q_ln.weight\n",
      "transformer.blocks.35.ffn.0.bias\n",
      "transformer.blocks.35.ffn.0.weight\n",
      "transformer.blocks.35.ffn.1.weight\n",
      "transformer.blocks.35.ffn.3.weight\n",
      "transformer.blocks.36.attn.k_ln.weight\n",
      "transformer.blocks.36.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.36.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.36.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.36.attn.out_proj.weight\n",
      "transformer.blocks.36.attn.q_ln.weight\n",
      "transformer.blocks.36.ffn.0.bias\n",
      "transformer.blocks.36.ffn.0.weight\n",
      "transformer.blocks.36.ffn.1.weight\n",
      "transformer.blocks.36.ffn.3.weight\n",
      "transformer.blocks.37.attn.k_ln.weight\n",
      "transformer.blocks.37.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.37.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.37.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.37.attn.out_proj.weight\n",
      "transformer.blocks.37.attn.q_ln.weight\n",
      "transformer.blocks.37.ffn.0.bias\n",
      "transformer.blocks.37.ffn.0.weight\n",
      "transformer.blocks.37.ffn.1.weight\n",
      "transformer.blocks.37.ffn.3.weight\n",
      "transformer.blocks.38.attn.k_ln.weight\n",
      "transformer.blocks.38.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.38.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.38.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.38.attn.out_proj.weight\n",
      "transformer.blocks.38.attn.q_ln.weight\n",
      "transformer.blocks.38.ffn.0.bias\n",
      "transformer.blocks.38.ffn.0.weight\n",
      "transformer.blocks.38.ffn.1.weight\n",
      "transformer.blocks.38.ffn.3.weight\n",
      "transformer.blocks.39.attn.k_ln.weight\n",
      "transformer.blocks.39.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.39.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.39.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.39.attn.out_proj.weight\n",
      "transformer.blocks.39.attn.q_ln.weight\n",
      "transformer.blocks.39.ffn.0.bias\n",
      "transformer.blocks.39.ffn.0.weight\n",
      "transformer.blocks.39.ffn.1.weight\n",
      "transformer.blocks.39.ffn.3.weight\n",
      "transformer.blocks.4.attn.k_ln.weight\n",
      "transformer.blocks.4.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.4.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.4.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.4.attn.out_proj.weight\n",
      "transformer.blocks.4.attn.q_ln.weight\n",
      "transformer.blocks.4.ffn.0.bias\n",
      "transformer.blocks.4.ffn.0.weight\n",
      "transformer.blocks.4.ffn.1.weight\n",
      "transformer.blocks.4.ffn.3.weight\n",
      "transformer.blocks.40.attn.k_ln.weight\n",
      "transformer.blocks.40.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.40.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.40.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.40.attn.out_proj.weight\n",
      "transformer.blocks.40.attn.q_ln.weight\n",
      "transformer.blocks.40.ffn.0.bias\n",
      "transformer.blocks.40.ffn.0.weight\n",
      "transformer.blocks.40.ffn.1.weight\n",
      "transformer.blocks.40.ffn.3.weight\n",
      "transformer.blocks.41.attn.k_ln.weight\n",
      "transformer.blocks.41.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.41.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.41.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.41.attn.out_proj.weight\n",
      "transformer.blocks.41.attn.q_ln.weight\n",
      "transformer.blocks.41.ffn.0.bias\n",
      "transformer.blocks.41.ffn.0.weight\n",
      "transformer.blocks.41.ffn.1.weight\n",
      "transformer.blocks.41.ffn.3.weight\n",
      "transformer.blocks.42.attn.k_ln.weight\n",
      "transformer.blocks.42.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.42.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.42.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.42.attn.out_proj.weight\n",
      "transformer.blocks.42.attn.q_ln.weight\n",
      "transformer.blocks.42.ffn.0.bias\n",
      "transformer.blocks.42.ffn.0.weight\n",
      "transformer.blocks.42.ffn.1.weight\n",
      "transformer.blocks.42.ffn.3.weight\n",
      "transformer.blocks.43.attn.k_ln.weight\n",
      "transformer.blocks.43.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.43.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.43.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.43.attn.out_proj.weight\n",
      "transformer.blocks.43.attn.q_ln.weight\n",
      "transformer.blocks.43.ffn.0.bias\n",
      "transformer.blocks.43.ffn.0.weight\n",
      "transformer.blocks.43.ffn.1.weight\n",
      "transformer.blocks.43.ffn.3.weight\n",
      "transformer.blocks.44.attn.k_ln.weight\n",
      "transformer.blocks.44.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.44.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.44.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.44.attn.out_proj.weight\n",
      "transformer.blocks.44.attn.q_ln.weight\n",
      "transformer.blocks.44.ffn.0.bias\n",
      "transformer.blocks.44.ffn.0.weight\n",
      "transformer.blocks.44.ffn.1.weight\n",
      "transformer.blocks.44.ffn.3.weight\n",
      "transformer.blocks.45.attn.k_ln.weight\n",
      "transformer.blocks.45.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.45.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.45.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.45.attn.out_proj.weight\n",
      "transformer.blocks.45.attn.q_ln.weight\n",
      "transformer.blocks.45.ffn.0.bias\n",
      "transformer.blocks.45.ffn.0.weight\n",
      "transformer.blocks.45.ffn.1.weight\n",
      "transformer.blocks.45.ffn.3.weight\n",
      "transformer.blocks.46.attn.k_ln.weight\n",
      "transformer.blocks.46.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.46.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.46.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.46.attn.out_proj.weight\n",
      "transformer.blocks.46.attn.q_ln.weight\n",
      "transformer.blocks.46.ffn.0.bias\n",
      "transformer.blocks.46.ffn.0.weight\n",
      "transformer.blocks.46.ffn.1.weight\n",
      "transformer.blocks.46.ffn.3.weight\n",
      "transformer.blocks.47.attn.k_ln.weight\n",
      "transformer.blocks.47.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.47.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.47.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.47.attn.out_proj.weight\n",
      "transformer.blocks.47.attn.q_ln.weight\n",
      "transformer.blocks.47.ffn.0.bias\n",
      "transformer.blocks.47.ffn.0.weight\n",
      "transformer.blocks.47.ffn.1.weight\n",
      "transformer.blocks.47.ffn.3.weight\n",
      "transformer.blocks.5.attn.k_ln.weight\n",
      "transformer.blocks.5.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.5.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.5.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.5.attn.out_proj.weight\n",
      "transformer.blocks.5.attn.q_ln.weight\n",
      "transformer.blocks.5.ffn.0.bias\n",
      "transformer.blocks.5.ffn.0.weight\n",
      "transformer.blocks.5.ffn.1.weight\n",
      "transformer.blocks.5.ffn.3.weight\n",
      "transformer.blocks.6.attn.k_ln.weight\n",
      "transformer.blocks.6.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.6.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.6.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.6.attn.out_proj.weight\n",
      "transformer.blocks.6.attn.q_ln.weight\n",
      "transformer.blocks.6.ffn.0.bias\n",
      "transformer.blocks.6.ffn.0.weight\n",
      "transformer.blocks.6.ffn.1.weight\n",
      "transformer.blocks.6.ffn.3.weight\n",
      "transformer.blocks.7.attn.k_ln.weight\n",
      "transformer.blocks.7.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.7.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.7.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.7.attn.out_proj.weight\n",
      "transformer.blocks.7.attn.q_ln.weight\n",
      "transformer.blocks.7.ffn.0.bias\n",
      "transformer.blocks.7.ffn.0.weight\n",
      "transformer.blocks.7.ffn.1.weight\n",
      "transformer.blocks.7.ffn.3.weight\n",
      "transformer.blocks.8.attn.k_ln.weight\n",
      "transformer.blocks.8.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.8.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.8.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.8.attn.out_proj.weight\n",
      "transformer.blocks.8.attn.q_ln.weight\n",
      "transformer.blocks.8.ffn.0.bias\n",
      "transformer.blocks.8.ffn.0.weight\n",
      "transformer.blocks.8.ffn.1.weight\n",
      "transformer.blocks.8.ffn.3.weight\n",
      "transformer.blocks.9.attn.k_ln.weight\n",
      "transformer.blocks.9.attn.layernorm_qkv.0.bias\n",
      "transformer.blocks.9.attn.layernorm_qkv.0.weight\n",
      "transformer.blocks.9.attn.layernorm_qkv.1.weight\n",
      "transformer.blocks.9.attn.out_proj.weight\n",
      "transformer.blocks.9.attn.q_ln.weight\n",
      "transformer.blocks.9.ffn.0.bias\n",
      "transformer.blocks.9.ffn.0.weight\n",
      "transformer.blocks.9.ffn.1.weight\n",
      "transformer.blocks.9.ffn.3.weight\n",
      "transformer.norm.weight\n",
      "Saved backbone-only model to: /home/jupyter/DATA/evqlv-dev/model-weights/esm3_backbone/esm3_backbone_model.pt\n"
     ]
    }
   ],
   "source": [
    "# Create a backbone-only state dict by removing all keys associated with output heads.\n",
    "# In your model, the output heads are under keys starting with \"output_heads.\"\n",
    "backbone_state_dict = {\n",
    "    key: value for key, value in model.state_dict().items()\n",
    "    if not key.startswith(\"output_heads\")\n",
    "}\n",
    "\n",
    "# Optionally, you can verify which keys remain:\n",
    "print(\"Backbone keys:\")\n",
    "for key in sorted(backbone_state_dict.keys()):\n",
    "    print(key)\n",
    "\n",
    "# Save the backbone-only state dict to a custom directory.\n",
    "backbone_save_dir = \"/home/jupyter/DATA/evqlv-dev/model-weights/esm3_backbone\"\n",
    "os.makedirs(backbone_save_dir, exist_ok=True)\n",
    "backbone_save_path = os.path.join(backbone_save_dir, \"esm3_backbone_model.pt\")\n",
    "torch.save(backbone_model, backbone_save_path)\n",
    "print(\"Saved backbone-only model to:\", backbone_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b221ce3e-3a86-4039-9dfd-f74fb83e1339",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m123"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13 (Local)",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
